{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "508a9239-601b-4d6c-9d55-7360df7155f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 29years old\n",
    "\n",
    "# Handling\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import tkinter as tk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import chardet\n",
    "\n",
    "# Natural Language Processing\n",
    "from collections import Counter \n",
    "from konlpy.tag import Okt, Komoran, Hannanum, Kkma, Mecab\n",
    "\n",
    "# Visualize and System\n",
    "from tkinter import filedialog, messagebox, ttk, StringVar\n",
    "from matplotlib.path import Path\n",
    "\n",
    "# Module\n",
    "from loadDatabase import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05750995-1d3d-4808-89a8-79675e155df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 형태소 분석기 설정 \n",
    "\n",
    "# 1. 단어 통계를 위한 Counter 객체 생성\n",
    "word_counter = Counter()\n",
    "\n",
    "# 2. 형태소 분석기(Mecab 로드)\n",
    "mecab = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "\n",
    "# 3. 형태소 분석기 초기화\n",
    "morpheme_analyzers = {\n",
    "    #\"Okt\": Okt(),\n",
    "    #\"Komoran\": Komoran(),\n",
    "    #\"Hannanum\": Hannanum(),\n",
    "    #\"Kkma\": Kkma(),\n",
    "    'Mecab': mecab }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bb742ee-2d0b-462c-b2e0-81ba4356f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(s, n):\n",
    "    tokens = s.split()\n",
    "    \n",
    "    n_grams_dataset = [\" \".join(tokens[i:i + n]) for i in range(len(tokens) - n + 1)]\n",
    "   \n",
    "    return n_grams_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2abdd33-7c98-4856-8bf2-83c6d8204bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_generate_ngrams(sentences,n):\n",
    "    ngram_start_time = time.time()  # 함수 실행 전 시간 측정\n",
    "\n",
    "    ngrams_comprehensions = [ngram for sentence in sentences['analyzed'] for ngram in generate_ngrams(sentence, n)]\n",
    "    \n",
    "    n_gram_dataset = pd.DataFrame(ngrams_comprehensions,columns=['n_grams']).reset_index()\n",
    "    n_gram_dataset['date'] = sentences['date']\n",
    "    n_gram_dataset['topic'] = sentences['topic']\n",
    "\n",
    "    n_gram_dataset = n_gram_dataset.dropna()\n",
    "\n",
    "    ngram_end_time = time.time()  # 함수 실행 후 시간 측정\n",
    "    execution_time = ngram_end_time - ngram_start_time  # 실행 시간 계산\n",
    "    print(f\"엔 그램 함수_Execution time: {execution_time:.6f} seconds\")\n",
    "\n",
    "    return n_gram_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23e9ad24-f239-4477-8d36-a0ee8d79531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 문장 품사 태깅함수 구현\n",
    "\n",
    "def tag_part_sentence_pre():\n",
    "\n",
    "    tag_start_time = time.time()  # 함수 실행 전 시간 측정\n",
    "    \n",
    "    # 분석 딕셔너리 \n",
    "    sentence_datas,folder_path = extract_materials_be_analyzed() \n",
    "    \n",
    "    # 어절 n_grams 생성\n",
    "    separate_sentence_to_phrase(sentence_datas,folder_path)\n",
    "\n",
    "    # n_grams 생성 \n",
    "    n_grams = [1,2,3]\n",
    "\n",
    "    # 아웃풋 딕셔너리 생성\n",
    "    all_sentences = {'original': [], 'analyzed': []}\n",
    "\n",
    "    for morpheme_name in morpheme_analyzers.keys():\n",
    "\n",
    "        morphem_repetition_start_time = time.time() \n",
    "        \n",
    "        sentences = sentence_datas['Sentence']\n",
    "        \n",
    "        for sentence in tqdm(sentences):\n",
    "          \n",
    "            # 선택한 형태소 분석기로 문장을 형태소 분석합니다.\n",
    "            morphemes = morpheme_analyzers[morpheme_name].pos(sentence)\n",
    "\n",
    "            # 제외 단어 목록에 포함되지 않은 형태소만 추가합니다.\n",
    "            filtered_morphemes = [f\"{word}/{tag}\" for word, tag in morphemes]\n",
    "\n",
    "            # 문장을 형태소 분석된 형태로 변환합니다.\n",
    "            analyzed_sentence = ' '.join(filtered_morphemes)\n",
    "\n",
    "            # 기존 문장 저장\n",
    "            all_sentences['original'].append(sentence)\n",
    "    \n",
    "             # 분석된 문장 저장\n",
    "            all_sentences['analyzed'].append(analyzed_sentence)\n",
    "        \n",
    "        # n-gram을 생성합니다.\n",
    "        for n_gram in n_grams:\n",
    "            \n",
    "            ngrams = apply_generate_ngrams(all_sentences['analyzed'], int(n_gram))\n",
    "\n",
    "            # n_counts 생성\n",
    "            ngrams_count = count_n_grams(ngrams)\n",
    "\n",
    "            ngrams_count.to_csv(f'{folder_path}\\\\morpheme_{morpheme_name}_ngram_{n_gram}.csv', index=False)\n",
    "\n",
    "        morphem_repetition_end_time = time.time()  # 함수 실행 후 시간 측정\n",
    "        execution_time = morphem_repetition_end_time - morphem_repetition_start_time  # 실행 시간 계산\n",
    "        print(f\"{morpheme_name}_반복문_Execution time: {execution_time:.6f} seconds\")\n",
    "    \n",
    "    tag_end_time = time.time()  # 함수 실행 후 시간 측정\n",
    "    execution_time = tag_end_time - tag_start_time  # 실행 시간 계산\n",
    "    print(f\"형태소 + 어절 함수_Execution time: {execution_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40494cde-1818-462c-892c-58eba1451a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 문장 품사 태깅함수 구현\n",
    "\n",
    "# 형태소 분석 및 필터링 함수 정의\n",
    "def analyze_sentence(sentence, morpheme_analyzer):\n",
    "    morphemes = morpheme_analyzer.pos(sentence)\n",
    "    filtered_morphemes = [f\"{word}/{tag}\" for word, tag in morphemes]\n",
    "    return ' '.join(filtered_morphemes)\n",
    "\n",
    "\n",
    "def tag_part_sentence():\n",
    "\n",
    "    morphem_dataset = []\n",
    "\n",
    "    tag_start_time = time.time()  # 함수 실행 전 시간 측정\n",
    "    \n",
    "    # 분석 딕셔너리 \n",
    "    sentence_datas,folder_path,middle_path = extract_materials_be_analyzed() \n",
    "    \n",
    "    # 어절 n_grams 생성\n",
    "    phrase_dataset = separate_sentence_to_phrase(sentence_datas,folder_path,middle_path)\n",
    "\n",
    "    # n_grams 생성 \n",
    "    n_grams = [1,2,3]\n",
    "\n",
    "    for morpheme_name in morpheme_analyzers.keys():\n",
    "        # 아웃풋 딕셔너리 생성\n",
    "        all_sentences = {'date':[],'topic':[],'analyzed':[]}\n",
    "        \n",
    "        # 형태소 분석기 인스턴스 가져오기\n",
    "        morpheme_analyzer = morpheme_analyzers[morpheme_name]\n",
    "\n",
    "        morphem_repetition_start_time = time.time() \n",
    "        \n",
    "        for index,row in tqdm(sentence_datas.iterrows()):\n",
    "            \n",
    "            date = row['date']\n",
    "            topic = row['topic']\n",
    "            sentences = row['sentences']\n",
    "\n",
    "            for sentence in sentences:\n",
    "\n",
    "                analyzed_sentence = analyze_sentence(sentence, morpheme_analyzer)\n",
    "\n",
    "                # 기존 문장 저장\n",
    "                all_sentences['analyzed'].append(analyzed_sentence)\n",
    "                # date, topic 설정\n",
    "                all_sentences['date'].append(date)\n",
    "                all_sentences['topic'].append(topic)\n",
    "\n",
    "        all_sentences = pd.DataFrame(all_sentences)   \n",
    "\n",
    "        # n_counts 생성\n",
    "        output_dataset = group_up(all_sentences,folder_path,morpheme_name,middle_path)\n",
    "\n",
    "        output_dataset_concat = pd.concat(output_dataset)\n",
    "\n",
    "        morphem_dataset.append(output_dataset_concat)\n",
    "\n",
    "        morphem_repetition_end_time = time.time()  # 함수 실행 후 시간 측정\n",
    "        execution_time = morphem_repetition_end_time - morphem_repetition_start_time  # 실행 시간 계산\n",
    "        print(f\"{morpheme_name}_반복문_Execution time: {execution_time:.6f} seconds\")\n",
    "\n",
    "\n",
    "    morphem_dataset_concat = pd.concat(morphem_dataset)\n",
    "\n",
    "    final_concat = pd.concat([phrase_dataset,morphem_dataset_concat],ignore_index=True)\n",
    "        \n",
    "    final_concat = final_concat.reset_index()\n",
    "\n",
    "    final_concat.columns=['STAT_NGRAM_NO','CORPUS_CL','ANALS_YEAR','ANALS_TOPIC','MORPHEME','NGRAM_TY','NGRAM_ORDR','NGRAM_FREQUENCY','NGRAM_VAL']\n",
    "\n",
    "    #morphem_dataset_concat.to_csv(f\"{folder_path}\\\\pharse_num.csv\",index=False)\n",
    "    #phrase_dataset.to_csv(f\"{folder_path}\\\\morphems_num.csv\",index=False)\n",
    "    #final_concat.to_csv(f\"{folder_path}\\\\final_concat.csv\",index=False)\n",
    "    load_database(final_concat,'corpus_stat_ngram_pra')\n",
    "\n",
    "    \n",
    "    tag_end_time = time.time()  # 함수 실행 후 시간 측정\n",
    "    execution_time = tag_end_time - tag_start_time  # 실행 시간 계산\n",
    "    print(f\"형태소 + 어절 함수_Execution time: {execution_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a4e8140-d036-4ee1-95af-4eaf053cf5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_sentence_to_phrase(sentence_datas,folder_path,middle_path):\n",
    "\n",
    "    phrase_start_time = time.time()  # 함수 실행 전 시간 측정\n",
    "\n",
    "    # 아웃풋 딕셔너리 생성\n",
    "    all_sentences = {'date':[],'topic':[],'analyzed':[]}\n",
    "    \n",
    "    for index,row in tqdm(sentence_datas.iterrows()):\n",
    "        date = row['date']\n",
    "        topic = row['topic']\n",
    "        sentences = row['sentences']\n",
    "\n",
    "        for sentence in sentences:\n",
    "                \n",
    "            # 선택한 형태소 분석기로 문장을 형태소 분석합니다.\n",
    "            morphemes = sentence.split(' ')\n",
    "            # 제외 단어 목록에 포함되지 않은 형태소만 추가합니다.\n",
    "            filtered_morphemes = [f\"{word}\" for word in morphemes]\n",
    "\n",
    "            # 문장을 형태소 분석된 형태로 변환합니다.\n",
    "            analyzed_sentence = ' '.join(filtered_morphemes)\n",
    "            \n",
    "            all_sentences['analyzed'].append(analyzed_sentence)\n",
    "            all_sentences['date'].append(date)\n",
    "            all_sentences['topic'].append(topic)\n",
    "\n",
    "    all_sentences = pd.DataFrame(all_sentences)\n",
    "\n",
    "    # n_counts 생성\n",
    "    phrase_data = group_up(all_sentences,folder_path,None,middle_path)\n",
    "\n",
    "    phrase = pd.concat(phrase_data)\n",
    "    \n",
    "    phrase_end_time = time.time()  # 함수 실행 후 시간 측정\n",
    "    execution_time = phrase_end_time - phrase_start_time  # 실행 시간 계산\n",
    "    print(f\"어절 함수_Execution time: {execution_time:.6f} seconds\")\n",
    "\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc5362c-cef5-4589-8a66-a9992081b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_up(data,folder_path,morpheme_name,middle_path):\n",
    "\n",
    "    phrase_data = []\n",
    "\n",
    "    dates = data['date'].unique()\n",
    "    topics = data['topic'].unique()\n",
    "\n",
    "    # n_grams 생성 \n",
    "    n_grams = [1,2,3]\n",
    "\n",
    "    for n_gram in n_grams:\n",
    "        for date in dates:\n",
    "            for topic in topics:\n",
    "                \n",
    "                selected_sentences = data[(data['date']==date) & (data['topic']==topic)]\n",
    "\n",
    "                # n-gram을 생성합니다.\n",
    "                ngrams = apply_generate_ngrams(selected_sentences, int(n_gram))\n",
    "            \n",
    "                ngrams_count = count_n_grams(ngrams)\n",
    "\n",
    "                ouput_dataset = clean_up_dataframes(ngrams_count,date,topic,morpheme_name,n_gram,middle_path)\n",
    "\n",
    "                ouput_dataset = ouput_dataset[['CORPUS_CL','ANALS_YEAR','ANALS_TOPIC','MORPHEME','NGRAM_TY','NGRAM_ORDR','NGRAM_FREQUENCY','NGRAM_VAL']]\n",
    "\n",
    "                phrase_data.append(ouput_dataset)\n",
    "                \n",
    "            #ngrams_count.to_csv(f\"{folder_path}\\\\test_num_{num}.csv\",index=False)\n",
    "\n",
    "    return phrase_data              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ed51b06-dd73-488c-8e38-a84744c6f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(words):\n",
    "    # 단어 카운트\n",
    "    word_count = Counter(words['n_grams'])\n",
    "\n",
    "    # 데이터 프레임 \n",
    "    word_count_data = pd.DataFrame(list(word_count.items()), columns=['NGRAM_VAL','NGRAM_FREQUENCY']).dropna()\n",
    "\n",
    "    # 순서정렬\n",
    "    word_count_data = word_count_data.sort_values(by='NGRAM_FREQUENCY', ascending=False)[:200]\n",
    "\n",
    "    # 인덱스 제거\n",
    "    word_count_data = word_count_data.reset_index(drop=True)\n",
    "\n",
    "    # 순서 인덱스 생성\n",
    "    word_count_data = word_count_data.reset_index()\n",
    "\n",
    "    word_count_data['index'] = word_count_data['index'] + 1\n",
    "\n",
    "    # 컬럼 이름 변경\n",
    "    word_count_data.columns = ['NGRAM_ORDR','NGRAM_VAL','NGRAM_FREQUENCY']\n",
    "    \n",
    "    return word_count_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef7e47ac-e051-4d3c-bd90-5311ce054aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_dataframes(ngrams_count,date,topic,morpheme_name,n_gram,middle_path):\n",
    "\n",
    "    # 변수 생성\n",
    "    ngrams_count['ANALS_YEAR'] = date\n",
    "    ngrams_count['ANALS_TOPIC'] = topic\n",
    "    ngrams_count['NGRAM_TY'] = n_gram\n",
    "\n",
    "\n",
    "    # 말뭉치 종류 생성\n",
    "    if middle_path!='utterance':\n",
    "        ngrams_count['CORPUS_CL'] = 'newspaper'\n",
    "    else:\n",
    "        ngrams_count['CORPUS_CL'] = 'dialogue'\n",
    "    \n",
    "    # 형태소 변수 생성\n",
    "    if morpheme_name:\n",
    "        ngrams_count['MORPHEME'] = morpheme_name\n",
    "    else:\n",
    "        ngrams_count['MORPHEME'] = 'pharse'\n",
    "\n",
    "    return ngrams_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "458b3e7a-7cfe-4b7f-aec4-b9cc6958ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 제이슨 필터링 함수 구현\n",
    "\n",
    "def filter_jason_folder(folder_path):\n",
    "    \"\"\" 필터 값에 맞는 JASON 데이터 추출하기 \n",
    "\n",
    "    파라미터: 폴더 경로, 필터 키, 필터 값\n",
    "\n",
    "    반환 값: 필터 값에 맞는 JASON을 추가한 폴더 \n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()  # 함수 실행 전 시간 측정\n",
    "    \n",
    "    # 제이슨 폴더 생성\n",
    "    jason_folder = []\n",
    "\n",
    "    # 초기 값 설정\n",
    "    cheked_file_path = None \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if (filename.endswith('.json')) | (filename.endswith('.JSON')):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            jason_folder.append(file_path)\n",
    "\n",
    "    end_time = time.time()  # 함수 실행 후 시간 측정\n",
    "    execution_time = end_time - start_time  # 실행 시간 계산\n",
    "    print(f\"폴더 필터링 함수_Execution time: {execution_time:.6f} seconds\")\n",
    "\n",
    "    return jason_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce0ab4a5-94c9-43c2-858b-7929f3251513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 리스트 틀 정제 함수 \n",
    "\n",
    "def flatten_list(nested_list):\n",
    "    flat_list = []\n",
    "    for element in nested_list:\n",
    "        if isinstance(element, list):  # 요소가 리스트인 경우, 재귀 호출\n",
    "            flat_list.extend(flatten_list(element))\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce8f7ac5-e686-4513-8ff0-ee4b50f3c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: Tag 내용 추출 함수 구현\n",
    "\n",
    "def extract_tag_pre(data,path_elements):\n",
    "    \"\"\" JASON 데이터의 TAG 내용 추출\n",
    "    \n",
    "    파라미터: JASN 데이터, 테그 리스트\n",
    "\n",
    "    반환 값: 테그 분석 내용    \n",
    "    \"\"\"\n",
    "    # tag 원소를 담을 그릇\n",
    "    tag_bowl = []\n",
    "    last_element = path_elements[-1]\n",
    "    \n",
    "    try:\n",
    "        # 첫 번째 경로 요소를 추출\n",
    "        first_element = path_elements[0]\n",
    "        # 현재 경로 요소가 리스트를 요구하는 경우\n",
    "        if isinstance(data, list):\n",
    "            if (first_element==last_element):\n",
    "                for num in range(len(data)):\n",
    "                    val = data[num][first_element]\n",
    "                    tag_bowl.append(val)             \n",
    "                return tag_bowl\n",
    "            else:      \n",
    "                # 리스트의 각 요소에 대해 재귀적으로 함수를 호출\n",
    "                result = [extract_tag(item, path_elements) for item in data]\n",
    "                # None 값을 제외한 결과만 필터링\n",
    "                return [item for item in result if item is not None]       \n",
    "        # 현재 데이터가 딕셔너리이고 경로 요소가 키로 존재하는 경우\n",
    "        elif isinstance(data, dict) and first_element in data:      \n",
    "            # 다음 경로 요소로 재귀적으로 함수를 호출    \n",
    "            return extract_tag(data[first_element], path_elements[1:])      \n",
    "        elif (first_element==last_element):\n",
    "            return data[first_element]           \n",
    "    except KeyError as e:\n",
    "            print(f\"Path not found in the JSON structure: {e}\")\n",
    "    except Exception as e:\n",
    "            print(f\"An error occurred: {e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f02a629b-b63b-47cb-a290-6ecb477ec867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: Tag 내용 추출 함수 구현\n",
    "# development_progress\n",
    "def extract_tag(data,path_elements):\n",
    "    \"\"\" JASON 데이터의 TAG 내용 추출\n",
    "    \n",
    "    파라미터: JASN 데이터, 테그 리스트\n",
    "\n",
    "    반환 값: 테그 분석 내용    \n",
    "    \"\"\"\n",
    "    # tag 원소를 담을 그릇\n",
    "    tag_bowl = []\n",
    "    start_path_element = path_elements[0]\n",
    "    middle_path_element = path_elements[1]\n",
    "    end_path_element = path_elements[2]\n",
    "    \n",
    "    for doc in data[start_path_element]:\n",
    "\n",
    "        doct_dict = {'date':'','topic':'','sentences':[]}\n",
    "\n",
    "        date = doc['metadata']['date']\n",
    "        topic = doc['metadata']['topic']\n",
    "\n",
    "        if middle_path_element !='utterance':\n",
    "            paragraphs =  doc['paragraph']\n",
    "        else:\n",
    "            paragraphs = doc['utterance']\n",
    "\n",
    "        doct_dict['date'] = date[:4]\n",
    "        doct_dict['topic'] = topic\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            doct_dict['sentences'].append(paragraph['form'])\n",
    "\n",
    "        tag_bowl.append(doct_dict)\n",
    "\n",
    "        data = pd.DataFrame(tag_bowl)\n",
    "        \n",
    "    return  data,middle_path_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "574fff6b-1d74-49fd-9963-626bba5c7c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Action: 폴더 필터링, 분석 테그 내용 컴바인 함수 구현 \n",
    "\n",
    "def extract_materials_be_analyzed():\n",
    "    \"\"\" 폴더 필터링, 분석 태그 내용 컴바인 함수 구현\n",
    "\n",
    "    반환 값: 테그 내용 리스트\n",
    "    \"\"\"\n",
    "    # 태그 설정\n",
    "    path_elements = user_input.get().split('.')\n",
    "    \n",
    "    if path_elements==[\"\"]:\n",
    "        messagebox.showinfo(\"Error\",\"태그 값을 입력하세요\")\n",
    "        return   \n",
    "    \n",
    "    # 폴더 경로 설정\n",
    "    folder_path = filedialog.askdirectory()\n",
    "    #print(\"현자 경로:\",folder_path)\n",
    "    \n",
    "    # 폴더에서 필터한 폴더를 반환\n",
    "    fited_jason_forder = filter_jason_folder(folder_path)\n",
    "    #print(f'제이슨 폴더: {fited_jason_forder}')\n",
    "\n",
    "    # 분석할 내용을 담을 딕셔너리 생성 \n",
    "    analysis_bowl = []\n",
    "\n",
    "    # 폴더를 순회하면서 분석 내용 추출\n",
    "    for jason_file in fited_jason_forder:\n",
    "\n",
    "        # 파일 인코딩 체크\n",
    "        file_encoding = detect_encoding(jason_file)\n",
    "        \n",
    "        with open(jason_file, 'r', encoding=file_encoding) as file:\n",
    "            # 해당 데이터 \n",
    "            data = json.load(file)\n",
    "            # 파일 이름 생성 \n",
    "            sentence_csv_path = create_file_name(folder_path,jason_file)\n",
    "             # 데이터 프레임 추출 \n",
    "            data,middle_path = extract_tag(data,path_elements) \n",
    "            # 데이터 리스트 추가\n",
    "            analysis_bowl.append(data)\n",
    "\n",
    "    # 데이터 병합\n",
    "    concat_data = pd.concat(analysis_bowl)\n",
    "\n",
    "    return concat_data,folder_path,middle_path         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21c588f9-fec2-4d1e-9072-c8977fd1a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 파일 이름 생성 함수\n",
    "\n",
    "def create_file_name(folder_path,jason_file):\n",
    "    \n",
    "    # 파일 이름만 추출\n",
    "    file_name_with_extension = os.path.basename(jason_file)\n",
    "\n",
    "    # 확장자 제거\n",
    "    file_name, _ = os.path.splitext(file_name_with_extension)\n",
    "    \n",
    "    sentence_csv_path = os.path.join(folder_path, f\"{file_name}.csv\")\n",
    "\n",
    "    return sentence_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd97c079-42ad-46db-b165-2cf1180eb58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: Jason 인코딩 감지 함수 \n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as file:  # 파일을 바이너리 모드로 열기\n",
    "        raw_data = file.read(10000)  # 파일의 첫 부분을 읽어 인코딩 감지 (전체 파일을 읽어도 되지만 메모리를 많이 사용할 수 있음)\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding = result['encoding']\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90cc5e9e-d362-4e94-b073-09fd234679d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_numbers(char):\n",
    "    return char.isdigit()\n",
    "\n",
    "def update_user_input(*args):\n",
    "    user_input.delete(0, 'end')\n",
    "    user_input.insert(0, tag_options[tag_variable.get()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e5e6d3a-1af0-472d-a6bb-383c2b9ff45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 스타일 설정\n",
    "\n",
    "def configure_styles():\n",
    "    style = ttk.Style()\n",
    "    style.theme_use('clam')  # 클램 테마는 더 현대적인 느낌을 줍니다.\n",
    "    style.configure('TLabel', font=('Arial', 10), background='white')\n",
    "    style.configure('TEntry', font=('Arial', 10), padding=5)\n",
    "    style.configure('TButton', font=('Arial', 10), padding=5)\n",
    "    style.configure('TCombobox', font=('Arial', 10), padding=5)\n",
    "    style.map('TCombobox', fieldbackground=[('readonly', 'white')],\n",
    "              selectbackground=[('readonly', 'white')],\n",
    "              selectforeground=[('readonly', 'black')])\n",
    "    style.configure('TFrame', background='white')  # 프레임 배경색 설정\n",
    "    style.configure('Horizontal.TProgressbar', background='#FA8072')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7799f8b7-b074-4271-aa1d-b882661e41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_table():\n",
    "    global canvas, word_counter\n",
    "\n",
    "    # Remove previous canvas if exists\n",
    "    if canvas is not None:\n",
    "        canvas.get_tk_widget().pack_forget()\n",
    "        canvas = None\n",
    "\n",
    "    # 표를 비웁니다.\n",
    "    # result_tree.delete(*result_tree.get_children())\n",
    "\n",
    "    # 단어 카운터를 초기화합니다.\n",
    "    word_counter = Counter()\n",
    "\n",
    "    # 프로그레스바를 0으로 초기화합니다.\n",
    "    #progress_bar['value'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d34fe5e3-0350-42a7-b5ca-1e3c3aaf3bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status_and_search():\n",
    "    \"\"\"\n",
    "    로딩 함수 구현\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 로딩 상태 업데이트\n",
    "        status_label.config(text=\"로딩 중...\")\n",
    "        root.update_idletasks()  # UI 업데이트 강제 실행\n",
    "\n",
    "        # 기존 검색 함수 호출\n",
    "        tag_part_sentence()\n",
    "\n",
    "        # 작업 완료 후 상태 업데이트\n",
    "        status_label.config(text=\"작업 완료\")\n",
    "        \n",
    "    except:\n",
    "        # 오류 발생 시 메시지 업데이트\n",
    "        status_label.config(text=\"작업 실패:\")\n",
    "        print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55834d-519a-420c-967f-5f382a5deb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더 필터링 함수_Execution time: 0.000950 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1129it [00:00, 14520.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "엔 그램 함수_Execution time: 0.017903 seconds\n",
      "엔 그램 함수_Execution time: 0.036905 seconds\n",
      "엔 그램 함수_Execution time: 0.003984 seconds\n",
      "엔 그램 함수_Execution time: 0.029185 seconds\n",
      "엔 그램 함수_Execution time: 0.010598 seconds\n",
      "엔 그램 함수_Execution time: 0.014030 seconds\n",
      "엔 그램 함수_Execution time: 0.002990 seconds\n",
      "엔 그램 함수_Execution time: 0.016152 seconds\n",
      "엔 그램 함수_Execution time: 0.025692 seconds\n",
      "엔 그램 함수_Execution time: 0.053081 seconds\n",
      "엔 그램 함수_Execution time: 0.005984 seconds\n",
      "엔 그램 함수_Execution time: 0.032217 seconds\n",
      "엔 그램 함수_Execution time: 0.010980 seconds\n",
      "엔 그램 함수_Execution time: 0.017016 seconds\n",
      "엔 그램 함수_Execution time: 0.003319 seconds\n",
      "엔 그램 함수_Execution time: 0.009324 seconds\n",
      "엔 그램 함수_Execution time: 0.019155 seconds\n",
      "엔 그램 함수_Execution time: 0.041511 seconds\n",
      "엔 그램 함수_Execution time: 0.004946 seconds\n",
      "엔 그램 함수_Execution time: 0.035482 seconds\n",
      "엔 그램 함수_Execution time: 0.009973 seconds\n",
      "엔 그램 함수_Execution time: 0.015957 seconds\n",
      "엔 그램 함수_Execution time: 0.002992 seconds\n",
      "엔 그램 함수_Execution time: 0.009974 seconds\n",
      "어절 함수_Execution time: 0.657879 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1129it [00:01, 744.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "엔 그램 함수_Execution time: 0.030919 seconds\n",
      "엔 그램 함수_Execution time: 0.074931 seconds\n",
      "엔 그램 함수_Execution time: 0.010095 seconds\n",
      "엔 그램 함수_Execution time: 0.054854 seconds\n",
      "엔 그램 함수_Execution time: 0.016954 seconds\n",
      "엔 그램 함수_Execution time: 0.030187 seconds\n",
      "엔 그램 함수_Execution time: 0.004175 seconds\n",
      "엔 그램 함수_Execution time: 0.016955 seconds\n",
      "엔 그램 함수_Execution time: 0.037899 seconds\n",
      "엔 그램 함수_Execution time: 0.092752 seconds\n",
      "엔 그램 함수_Execution time: 0.008976 seconds\n",
      "엔 그램 함수_Execution time: 0.065464 seconds\n",
      "엔 그램 함수_Execution time: 0.019946 seconds\n",
      "엔 그램 함수_Execution time: 0.032912 seconds\n",
      "엔 그램 함수_Execution time: 0.003989 seconds\n",
      "엔 그램 함수_Execution time: 0.014960 seconds\n",
      "엔 그램 함수_Execution time: 0.039894 seconds\n",
      "엔 그램 함수_Execution time: 0.100731 seconds\n",
      "엔 그램 함수_Execution time: 0.009974 seconds\n",
      "엔 그램 함수_Execution time: 0.066822 seconds\n",
      "엔 그램 함수_Execution time: 0.021942 seconds\n",
      "엔 그램 함수_Execution time: 0.033910 seconds\n",
      "엔 그램 함수_Execution time: 0.004986 seconds\n",
      "엔 그램 함수_Execution time: 0.020944 seconds\n",
      "Mecab_반복문_Execution time: 2.489860 seconds\n",
      "Connecting to the database...\n",
      "Creating table corpus_stat_ngram_pra...\n",
      "Table 'corpus_stat_ngram_pra' created.\n",
      "Inserting data into corpus_stat_ngram_pra...\n",
      "DataFrame has been successfully loaded into corpus_stat_ngram_pra table in the database.\n",
      "형태소 + 어절 함수_Execution time: 13.571871 seconds\n"
     ]
    }
   ],
   "source": [
    "# Action: GUI 생성\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"n-gram 및 형태소 분석기 v1.1\")\n",
    "# 프로그램의 고정된 크기\n",
    "program_width = 670\n",
    "program_height = 850\n",
    "\n",
    "# 화면의 중앙에 프로그램이 위치하도록 좌표를 계산합니다.\n",
    "screen_width = root.winfo_screenwidth()\n",
    "screen_height = root.winfo_screenheight()\n",
    "center_x = int((screen_width - program_width) / 2)\n",
    "center_y = int((screen_height - program_height) / 2)\n",
    "\n",
    "# 프로그램의 위치와 크기를 설정합니다.\n",
    "root.geometry(f'{program_width}x{program_height}+{center_x}+{center_y}')\n",
    "\n",
    "configure_styles()\n",
    "\n",
    "# 숫자 입력 확인을 위한 유효성 검사 커맨드 생성\n",
    "vcmd = root.register(only_numbers)\n",
    "\n",
    "# (1,1)\n",
    "tag_label = tk.Label(root, text=\"* 태그\", anchor='w')\n",
    "tag_label.grid(row=0, column=0, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (1,2)\n",
    "user_input = ttk.Entry(root)\n",
    "user_input.grid(row=0, column=1, sticky='ew', padx=10, pady=5)\n",
    "\n",
    "# (1,3)\n",
    "tag_menu_label = tk.Label(root, text=\"* 태그 선택\", anchor='w')\n",
    "tag_menu_label.grid(row=0, column=2, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (1,4)\n",
    "tag_variable = StringVar(root)\n",
    "tag_variable.trace(\"w\", update_user_input)\n",
    "tag_options = {\n",
    "    \"신문 말뭉치\": \"document.paragraph.form\",\n",
    "    \"일상 대화 말뭉치\": \"document.utterance.form\",\n",
    "    \"직접 입력\": \"\",\n",
    "}\n",
    "tag_menu = ttk.Combobox(root, textvariable=tag_variable, values=list(tag_options.keys()), state='readonly')\n",
    "tag_menu.grid(row=0, column=3, sticky='ew', padx=10, pady=5)\n",
    "tag_menu.set(\"직접 입력\")\n",
    "\n",
    "\n",
    "# (2,1)\n",
    "folder_button = ttk.Button(root, text=\"폴더 선택 및 분석\", command=update_status_and_search)\n",
    "folder_button.grid(row=2, column=2, sticky='ew', padx=10, pady=5)\n",
    "\n",
    "# (2,2)\n",
    "reset_button = ttk.Button(root, text=\"결과창 리셋\", command=reset_table)\n",
    "reset_button.grid(row=2, column=3, sticky='ew', padx=10, pady=5)\n",
    "\n",
    "# (3,1)\n",
    "# 상태 레이블\n",
    "status_label = tk.Label(root, text=\"\")\n",
    "status_label.place(relx=0.5, rely=0.5, anchor='center')\n",
    "\n",
    "\n",
    "root.grid_columnconfigure(1, weight=1)\n",
    "root.grid_rowconfigure(6, weight=1)\n",
    "root.grid_rowconfigure(7, weight=1)\n",
    "\n",
    "# 초기 캔버스 설정을 None으로 합니다. 'embed_figure' 함수 정의가 필요합니다.\n",
    "canvas = None\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_38",
   "language": "python",
   "name": "python_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
