{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "581356af-9fbd-4527-bb5b-a8b118671073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import ijson\n",
    "import csv\n",
    "import tkinter as tk\n",
    "import pandas as pd\n",
    "import chardet\n",
    "from openpyxl import Workbook \n",
    "\n",
    "# Natural Language processing 22 years old\n",
    "from collections import Counter \n",
    "from konlpy.tag import Okt, Komoran, Hannanum, Kkma, Mecab\n",
    "\n",
    "# Visualize \n",
    "from tkinter import filedialog, messagebox, ttk, StringVar\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "from matplotlib import font_manager, rc\n",
    "from matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd2db66-48cf-4cd5-8e08-680a24b38bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 형태소 분석기 설정 \n",
    "\n",
    "# 1. 단어 통계를 위한 Counter 객체 생성\n",
    "word_counter = Counter()\n",
    "\n",
    "# 2. 형태소 분석기(Mecab 로드)\n",
    "mecab = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "\n",
    "# 3. 형태소 분석기 초기화\n",
    "morpheme_analyzers = {\n",
    "    \"선택 없음\": None,\n",
    "    \"Okt\": Okt(),\n",
    "    \"Komoran\": Komoran(),\n",
    "    \"Hannanum\": Hannanum(),\n",
    "    \"Kkma\": Kkma(),\n",
    "    'Mecab': mecab }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d4a695-fffa-4208-93e2-6759d72d1208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 폰트 설정\n",
    "\n",
    "#if getattr(sys, 'frozen', False):  # 코드가 PyInstaller로 패키징된 경우\n",
    "#    base_path = sys._MEIPASS\n",
    "#else:\n",
    "#    base_path = os.path.dirname(__file__)\n",
    "\n",
    "base_path = os.getcwd()\n",
    "\n",
    "font_path = os.path.join(base_path, \"fonts\", \"malgun.ttf\")\n",
    "font_name = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c5b6656-f5cf-42d8-911c-92f5be525672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: ngrams 함수 생성\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    # Input: s = string, n = size of the ngram\n",
    "    # Output: list of ngrams\n",
    "    tokens = s.split()\n",
    "    \n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)]) \n",
    "    \n",
    "    n_grams_dataset = pd.DataFrame([\" \".join(ngram) for ngram in ngrams],columns=['n_grams'])\n",
    "\n",
    "    return n_grams_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac7634d1-530d-406b-b917-a4a9c08fdef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_generate_ngrams(sentences,n):\n",
    "    ngrams_list = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        #print(sentence)\n",
    "        n_grams_sentence = generate_ngrams(sentence,n)\n",
    "        ngrams_list.append(n_grams_sentence)\n",
    "\n",
    "    n_gram_dataset = pd.concat(ngrams_list).reset_index().dropna()\n",
    "    return n_gram_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66f45b0d-63b9-45a2-8d19-d3c1e705f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 콘코던스 단어 추적 함수 구현\n",
    "\n",
    "def track_down_concordance_words():\n",
    "    \n",
    "    # concordance_entry에서 단어 목록을 가져옵니다.\n",
    "    #concordance_words = concordance_entry.split('|')\n",
    "    concordance_words_get = concordance_entry.get()\n",
    "    concordance_words_get = replace_strip(concordance_words_get)\n",
    "    \n",
    "    if concordance_words_get != 'None':\n",
    "        concordance_words = concordance_words_get.split('|')\n",
    "        concordance_words = [word.strip() for word in concordance_words]\n",
    "\n",
    "        return  concordance_words\n",
    "\n",
    "    else:\n",
    "        return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8defdcf3-68d2-4650-8dc1-eedcbb11ad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 제외 단어 추적 함수 구현\n",
    "\n",
    "def track_down_exclude_words():\n",
    "\n",
    "    # 제외 단어 목록을 가져옵니다.\n",
    "    exclude_words = exclude_words_entry.get().split('|')\n",
    "    #exclude_words = exclude_words_entry.split('|')\n",
    "    exclude_words = [word.strip() for word in exclude_words]\n",
    "\n",
    "    return exclude_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7f3a413-d094-42ab-ab06-dc680444997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_strip(concordance_words_get):\n",
    "    if concordance_words_get.strip() == '':\n",
    "        return 'None'\n",
    "    else:\n",
    "        return concordance_words_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fedac186-d344-42b2-aff3-d58689a52d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 콘코던스 필터링 함수 구현\n",
    "\n",
    "def execute_concordence_sentence_only_fuction():\n",
    "    # 콘코던스 함수 실행\n",
    "    concordance_words_get = track_down_concordance_words()\n",
    "    # 문장 추출 함수 사용\n",
    "    analyzed_folder= extract_materials_be_analyzed() \n",
    "    #analyzed_folder = my_dict\n",
    "    # 폴더 경로 설정\n",
    "    folder_path = filedialog.askdirectory()\n",
    "    \n",
    "    sentence_csv_path = os.path.join(folder_path, f\"{'concordence'}.csv\")\n",
    "    sentence_csv_path = \"C:\\\\Users\\\\pc021\\\\Desktop\\\\JASON\\\\concordence.csv\"\n",
    "\n",
    "    concordence_dict = {}\n",
    "    \n",
    "    if (concordance_words_get) and ('None' not in concordance_words_get):\n",
    "        #dict_with_concordance={'original': [], 'analyzed': []}\n",
    "        list_with_concordence = [] \n",
    "        for key,value in analyzed_folder.items():\n",
    "            all_sentences = value['Sentence']\n",
    "            if any(any(con_word in sentence for con_word in concordance_words_get) for sentence in all_sentences):\n",
    "                print(\"포함된 문장이 존재함\")\n",
    "                for con_word in concordance_words_get:\n",
    "                    for sentence in all_sentences:\n",
    "                        if con_word in sentence:\n",
    "                            list_with_concordence.append(sentence)\n",
    "\n",
    "                # 콘코던스 필러링 데이터 프레임화\n",
    "                df_list_with_concordence = pd.DataFrame(list_with_concordence,columns=['Sentence'])\n",
    "                # 콘코던스 딕셔너리 생성\n",
    "                concordence_dict[key] = df_list_with_concordence \n",
    "            else:\n",
    "                print(\"포함된 문장이 없습니다.\")\n",
    "\n",
    "        return  concordence_dict\n",
    "    else:\n",
    "        print(\"콘코던스 키 입력하지 않음\")\n",
    "        #print(analyzed_folder)\n",
    "        return analyzed_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c21e4c66-0332-4878-815a-265b1b36bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 문장 품사 태깅함수 구현\n",
    "\n",
    "def tag_part_sentence():\n",
    "\n",
    "    # 분석 딕셔너리 \n",
    "    sentence_dicts = execute_concordence_sentence_only_fuction() \n",
    "    #print(sentence_dicts)\n",
    "    # 제외 단어 추척 함수 사용\n",
    "    exclude_words_entry = track_down_exclude_words()\n",
    "\n",
    "    # n_grams 생성 \n",
    "    n_gram = ngram_cnt_entry.get()\n",
    "\n",
    "    # 형태소 분석기 생성\n",
    "    morpheme_menu_get = morpheme_menu.get()\n",
    "    \n",
    "    for key,value in sentence_dicts.items():\n",
    "        # 아웃풋 딕셔너리 생성\n",
    "        all_sentences = {'original': [], 'analyzed': []}\n",
    "        \n",
    "        # 데이터 프레임 문장 추출\n",
    "        sentences = value['Sentence']\n",
    "        \n",
    "        # 제외 단어 추척 함수 사용\n",
    "        #exclude_words = track_down_exclude_words('먹었다')\n",
    "    \n",
    "        for sentence in sentences:\n",
    "            # 선택한 형태소 분석기로 문장을 형태소 분석합니다.\n",
    "            morphemes = morpheme_analyzers[morpheme_menu_get].pos(sentence)\n",
    "\n",
    "            # 제외 단어 목록에 포함되지 않은 형태소만 추가합니다.\n",
    "            filtered_morphemes = [f\"{word}/{tag}\" for word, tag in morphemes if word not in exclude_words_entry]\n",
    "\n",
    "            # 문장을 형태소 분석된 형태로 변환합니다.\n",
    "            analyzed_sentence = ' '.join(filtered_morphemes)\n",
    "\n",
    "            # 기존 문장 저장\n",
    "            all_sentences['original'].append(sentence)\n",
    "    \n",
    "            # 분석된 문장 저장\n",
    "            all_sentences['analyzed'].append(analyzed_sentence)\n",
    "\n",
    "        # n-gram을 생성합니다.\n",
    "        ngrams = apply_generate_ngrams(all_sentences['analyzed'], int(n_gram))\n",
    "\n",
    "        # n_counts 생성\n",
    "        ngrams_count = count_n_grams(ngrams)\n",
    "\n",
    "        # 분석 데이터 프레임 저장\n",
    "        pd.DataFrame(all_sentences).to_csv(f'{key}', index=False)\n",
    "        ngrams_count.to_csv(f'{key}_count.csv', index=False)\n",
    "\n",
    "    return pd.DataFrame(all_sentences),key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0ed8294c-af7c-421f-9de4-93c123245391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_sentence_to_phrase():\n",
    "    # 분석 딕셔너리 \n",
    "    sentence_dicts = execute_concordence_sentence_only_fuction() \n",
    "    \n",
    "    # 제외 단어 추척 함수 사용\n",
    "    exclude_words_entry = track_down_exclude_words()\n",
    "    \n",
    "    # n_grams 생성 \n",
    "    n_gram = ngram_cnt_entry.get()\n",
    "    \n",
    "    for key,value in sentence_dicts.items():\n",
    "        # 아웃풋 딕셔너리 생성\n",
    "        all_sentences = {'original': [], 'analyzed': []}\n",
    "        \n",
    "        # 데이터 프레임 문장 추출\n",
    "        sentences = value['Sentence']\n",
    "    \n",
    "        for sentence in sentences:\n",
    "            # 선택한 형태소 분석기로 문장을 형태소 분석합니다.\n",
    "            morphemes = sentence.split(' ')\n",
    "            # 제외 단어 목록에 포함되지 않은 형태소만 추가합니다.\n",
    "            filtered_morphemes = [f\"{word}\" for word in morphemes if word not in exclude_words_entry]\n",
    "\n",
    "            # 문장을 형태소 분석된 형태로 변환합니다.\n",
    "            analyzed_sentence = ' '.join(filtered_morphemes)\n",
    "\n",
    "            # 기존 문장 저장\n",
    "            all_sentences['original'].append(sentence)\n",
    "    \n",
    "            # 분석된 문장 저장\n",
    "            all_sentences['analyzed'].append(analyzed_sentence)\n",
    "\n",
    "        # n-gram을 생성합니다.\n",
    "        ngrams = apply_generate_ngrams(all_sentences['analyzed'], int(n_gram))\n",
    "\n",
    "        # n_counts 생성\n",
    "        ngrams_count = count_n_grams(ngrams)\n",
    "\n",
    "        # 분석 데이터 프레임 저장\n",
    "        pd.DataFrame(all_sentences).to_csv(f'{key}', index=False)\n",
    "        ngrams_count.to_csv(f'{key}_count.csv', index=False)\n",
    "\n",
    "    return pd.DataFrame(all_sentences),key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e4dd481-27f1-4173-86ad-565beadda602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(words):\n",
    "    # 단어 카운트\n",
    "    word_count = Counter(words['n_grams'])\n",
    "    \n",
    "    word_count_data = pd.DataFrame(list(word_count.items()), columns=['Word','Frequency']).dropna()\n",
    "\n",
    "    word_count_data = word_count_data.sort_values(by='Frequency', ascending=False)\n",
    "    \n",
    "    return word_count_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bb19008c-e7c2-4f25-b59c-07142375f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 제이슨 필터링 함수 구현\n",
    "\n",
    "def filter_jason_folder(folder_path,filter_key,expected_value):\n",
    "    \"\"\" 필터 값에 맞는 JASON 데이터 추출하기 \n",
    "\n",
    "    파라미터: 폴더 경로, 필터 키, 필터 값\n",
    "\n",
    "    반환 값: 필터 값에 맞는 JASON을 추가한 폴더 \n",
    "    \"\"\"\n",
    "    # 제이슨 폴더 생성\n",
    "    jason_folder = []\n",
    "    filter_jason_folder = []\n",
    "\n",
    "    #print(f\"현재 폴더:{folder_path}\") \n",
    "    # 초기 값 설정\n",
    "    cheked_file_path = None \n",
    "    #print(\"현재 폴더:\",folder_path)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if (filename.endswith('.json')) | (filename.endswith('.JSON')):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            jason_folder.append(file_path)\n",
    "            #print(f\"필터링 폴더:{jason_folder}\")\n",
    "            #파일 인코딩 체크\n",
    "            file_encoding = detect_encoding(file_path)\n",
    "            with open(file_path, 'r', encoding=file_encoding) as file:\n",
    "                data = json.load(file)\n",
    "                # 필터 키 존재한다면\n",
    "                if filter_key != None:\n",
    "                    cheked_file_path = inspect_jason(data,filter_key,expected_value,file_path)\n",
    "                # 만약 조건에 맞는 jason 파일이 있다면\n",
    "                if cheked_file_path:\n",
    "                    filter_jason_folder.append(cheked_file_path)\n",
    "                    \n",
    "    # 필터링 제이슨 폴터가 존재한다면            \n",
    "    if filter_jason_folder:\n",
    "        #print(\"필터링 조건 통과\")\n",
    "        return filter_jason_folder\n",
    "    else:\n",
    "        #print(\"조건 통과하지 않음\")\n",
    "        return jason_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "513dbf30-9b9c-4b51-8c37-9f55c7743ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: filter key, values를 이용한 jason 파일 필터링 \n",
    "\n",
    "def inspect_jason(jason_data,filter_key,expected_value,file_path):\n",
    "    \"\"\"\n",
    "    필터 값과 일치하는 JASON 데이터 검사\n",
    "\n",
    "    파라미터: 데이터, 필터 키, 필터 값, 데이터 경로\n",
    "\n",
    "    반환 값: 필터 값에 맞는 JASON 데이터 경로\n",
    "    \"\"\"\n",
    "    # 필터 키 분리\n",
    "    filter_key_list = filter_key.split('.')\n",
    "    \n",
    "    # 조건 실행\n",
    "    try:\n",
    "        for key in filter_key_list:\n",
    "            if isinstance(jason_data,dict) and key in jason_data:\n",
    "                jason_data = jason_data[key]\n",
    "            elif isinstance(jason_data,list):\n",
    "                 # 리스트의 경우, 리스트의 모든 요소를 포함하는 새 리스트를 생성\n",
    "                jason_data = [subvalue[key] for subvalue in jason_data if key in subvalue]\n",
    "            else:\n",
    "                raise KeyError(\"Key not found in the JSON structure.\")\n",
    "        # 최종 값을 확인\n",
    "        if isinstance(jason_data, list):\n",
    "            matches = [val for val in jason_data if expected_value in val]\n",
    "            if matches:\n",
    "                print(f\"Match found: {matches}\")\n",
    "                return file_path\n",
    "            else:\n",
    "                print(\"No match found.\")\n",
    "        else:\n",
    "            if expected_value in jason_data:\n",
    "                return file_path\n",
    "                print(f\"Match found: {jason_data}\")\n",
    "            else:\n",
    "                print(\"No match found.\")                \n",
    "\n",
    "    except KeyError as e:\n",
    "            print(f\"Path not found in the JSON structure: {e}\")\n",
    "    except Exception as e:\n",
    "            print(f\"An error occurred: {e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "497a31a9-bcf2-4f04-8ce7-6407337ec41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 리스트 틀 정제 함수 \n",
    "\n",
    "def flatten_list(nested_list):\n",
    "    flat_list = []\n",
    "    for element in nested_list:\n",
    "        if isinstance(element, list):  # 요소가 리스트인 경우, 재귀 호출\n",
    "            flat_list.extend(flatten_list(element))\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e827da6-cb28-41d3-be52-a32b823ae10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: Tag 내용 추출 함수 구현\n",
    "\n",
    "def extract_tag(data,path_elements):\n",
    "    \"\"\" JASON 데이터의 TAG 내용 추출\n",
    "    \n",
    "    파라미터: JASN 데이터, 테그 리스트\n",
    "\n",
    "    반환 값: 테그 분석 내용    \n",
    "    \"\"\"\n",
    "    # tag 원소를 담을 그릇\n",
    "    tag_bowl = []\n",
    "    last_element = path_elements[-1]\n",
    "    \n",
    "    try:\n",
    "        # 첫 번째 경로 요소를 추출\n",
    "        first_element = path_elements[0]\n",
    "        # 현재 경로 요소가 리스트를 요구하는 경우\n",
    "        if isinstance(data, list):\n",
    "            if (first_element==last_element):\n",
    "                for num in range(len(data)):\n",
    "                    val = data[num][first_element]\n",
    "                    tag_bowl.append(val)             \n",
    "                return tag_bowl\n",
    "            else:      \n",
    "                # 리스트의 각 요소에 대해 재귀적으로 함수를 호출\n",
    "                result = [extract_tag(item, path_elements) for item in data]\n",
    "                # None 값을 제외한 결과만 필터링\n",
    "                return [item for item in result if item is not None]       \n",
    "        # 현재 데이터가 딕셔너리이고 경로 요소가 키로 존재하는 경우\n",
    "        elif isinstance(data, dict) and first_element in data:      \n",
    "            # 다음 경로 요소로 재귀적으로 함수를 호출    \n",
    "            return extract_tag(data[first_element], path_elements[1:])      \n",
    "        elif (first_element==last_element):\n",
    "            return data[first_element]           \n",
    "    except KeyError as e:\n",
    "            print(f\"Path not found in the JSON structure: {e}\")\n",
    "    except Exception as e:\n",
    "            print(f\"An error occurred: {e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2f6e8aa4-57aa-48b0-a134-d856dd3641af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 폴더 필터링, 분석 테그 내용 컴바인 함수 구현 \n",
    "\n",
    "def extract_materials_be_analyzed():\n",
    "    \"\"\" 폴더 필터링, 분석 태그 내용 컴바인 함수 구현\n",
    "\n",
    "    반환 값: 테그 내용 리스트\n",
    "    \"\"\"\n",
    "    # 태그 설정\n",
    "    path_elements = user_input.get().split('.')\n",
    "    \n",
    "    # 폴더 경로 설정\n",
    "    folder_path = filedialog.askdirectory()\n",
    "    #print(\"현자 경로:\",folder_path)\n",
    "    \n",
    "    # 필터 키 설정\n",
    "    filter_key = filter_key_entry.get()\n",
    "\n",
    "    # 필터 값 설정\n",
    "    tag_name = filter_value_entry.get()\n",
    "    \n",
    "    # 폴더에서 필터한 폴더를 반환\n",
    "    fited_jason_forder = filter_jason_folder(folder_path,filter_key,tag_name)\n",
    "    #print(fited_jason_forder)\n",
    "\n",
    "    # 분석할 내용을 담을 딕셔너리 생성 \n",
    "    analysis_bowl = {}\n",
    "\n",
    "    # 폴더를 순회하면서 분석 내용 추출\n",
    "    for jason_file in fited_jason_forder:\n",
    "        #print(jason_file)\n",
    "        # 파일 인코딩 체크\n",
    "        file_encoding = detect_encoding(jason_file)\n",
    "        \n",
    "        with open(jason_file, 'r', encoding=file_encoding) as file:\n",
    "            # 해당 데이터 \n",
    "            data = json.load(file)\n",
    "\n",
    "            # 파일 이름 생성 \n",
    "            sentence_csv_path = create_file_name(folder_path,jason_file)\n",
    "             # 내용 추출 \n",
    "            rows_bowl = extract_tag(data,path_elements) \n",
    "            # 리스트 정제         \n",
    "            clean_list = flatten_list(rows_bowl)\n",
    "\n",
    "             # 데이터 프레임화\n",
    "            clean_list = pd.DataFrame(clean_list,columns=['Sentence'])\n",
    "\n",
    "            # 데이터 딕셔너리 추가\n",
    "            analysis_bowl[sentence_csv_path] = clean_list\n",
    "\n",
    "            #clean_list.to_csv(sentence_csv_path,encoding='utf-8',index=False)\n",
    "    #print(analysis_bowl)\n",
    "    return analysis_bowl       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0213fc18-af90-47f2-9a9b-5234875962f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 파일 이름 생성 함수\n",
    "\n",
    "def create_file_name(folder_path,jason_file):\n",
    "    \n",
    "    # 파일 이름만 추출\n",
    "    file_name_with_extension = os.path.basename(jason_file)\n",
    "\n",
    "    # 확장자 제거\n",
    "    file_name, _ = os.path.splitext(file_name_with_extension)\n",
    "    \n",
    "    sentence_csv_path = os.path.join(folder_path, f\"{file_name}.csv\")\n",
    "\n",
    "    return sentence_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f0dbacf-46c1-4245-88c3-e93c4c162fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: Jason 인코딩 감지 함수 \n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as file:  # 파일을 바이너리 모드로 열기\n",
    "        raw_data = file.read(10000)  # 파일의 첫 부분을 읽어 인코딩 감지 (전체 파일을 읽어도 되지만 메모리를 많이 사용할 수 있음)\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding = result['encoding']\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7917bc24-acd3-468a-a452-8b8fe507b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: Window size 함수 \n",
    "\n",
    "def cut_window_sizes(sentences,key):\n",
    "    # window_size 설정\n",
    "    window_size_get = int(window_cnt_entry.get())\n",
    "    #window_size_get = window_cnt_entry\n",
    "    \n",
    "    # 콘코던스 설정\n",
    "    concordance_entry_get = track_down_concordance_words()\n",
    "    #concordance_entry_get = concordance_entry\n",
    "\n",
    "    # 언어 단위 설정\n",
    "    linguistic_unit_menu_get = linguistic_unit_menu.get()\n",
    "    #linguistic_unit_menu_get = linguistic_unit_menu\n",
    "\n",
    "    # 형태소 분석기 설정\n",
    "    morpheme_menu_get = morpheme_menu.get()\n",
    "    #morpheme_menu_get = morpheme_menu\n",
    "\n",
    "    # 윈도우 사이즈 리스트 생성\n",
    "    window_size_list = [] \n",
    "    \n",
    "    # 문장 window_size 자르기\n",
    "    for language_sentence in sentences:\n",
    "\n",
    "        if linguistic_unit_menu_get == '어절':\n",
    "            # 문장을 공백 기준으로 단어로 분리\n",
    "            words = language_sentence.split()\n",
    "        \n",
    "        else:\n",
    "            words = morpheme_analyzers[morpheme_menu_get].morphs(language_sentence)\n",
    "    \n",
    "        #print(f'현재 문장 : {sentence}')\n",
    "        #print(f'현재 단어들 : {words}')\n",
    "        \n",
    "        for concordance in concordance_entry_get:\n",
    "            #print(f'현재 콘코던스 : {concordance}')\n",
    "            if concordance in language_sentence:\n",
    "                #print(\"조건 통과\")\n",
    "                target_index = words.index(concordance)\n",
    "                #print(f'타겟 인덱스 : {target_index}')\n",
    "\n",
    "                start_index = max(0, target_index - window_size_get)\n",
    "                end_index = min(len(words), target_index + window_size_get + 1)\n",
    "\n",
    "                window_sentence = ' '.join(words[start_index:end_index])\n",
    "\n",
    "                window_size_list.append(window_sentence)\n",
    "                #print(window_sentence)\n",
    "        \n",
    "    #return \n",
    "    window_size_dataset = pd.DataFrame(window_size_list,columns=['window_size_sentence'])\n",
    "    window_size_dataset.to_csv(f'{key}_window_size.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d099683f-4daa-44b4-bd04-ab9d281afe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_language_unit():\n",
    "    # 언어 단위 설정\n",
    "    linguistic_unit_menu_get = linguistic_unit_menu.get()\n",
    "    #linguistic_unit_menu_get = linguistic_unit_menu\n",
    "\n",
    "    if linguistic_unit_menu_get =='어절':\n",
    "        return separate_sentence_to_phrase()\n",
    "    else:\n",
    "        return tag_part_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "10fc1f24-14d3-4794-bda9-e6a920a7c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_final_language_analyzer():\n",
    "\n",
    "    language_unit = choose_language_unit()\n",
    "\n",
    "    senetence_dataset,key = language_unit\n",
    "\n",
    "    sentences = senetence_dataset['original']\n",
    "    \n",
    "    # 윈도우 사이즈 설정 \n",
    "    window_size_get = window_cnt_entry.get()\n",
    "    #window_size_get = 3\n",
    "    if window_size_get != '' :\n",
    "        cut_window_sizes(sentences,key)\n",
    "        return language_unit\n",
    "    else:\n",
    "        return language_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7ebc7b3-e9a2-409f-8476-4e02cf8626f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_numbers(char):\n",
    "    return char.isdigit()\n",
    "\n",
    "def update_user_input(*args):\n",
    "    user_input.delete(0, 'end')\n",
    "    user_input.insert(0, tag_options[tag_variable.get()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "697e0ac2-a8c9-4760-a1b6-585f28c2dd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 스타일 설정\n",
    "\n",
    "def configure_styles():\n",
    "    style = ttk.Style()\n",
    "    style.theme_use('clam')  # 클램 테마는 더 현대적인 느낌을 줍니다.\n",
    "    style.configure('TLabel', font=('Arial', 10), background='white')\n",
    "    style.configure('TEntry', font=('Arial', 10), padding=5)\n",
    "    style.configure('TButton', font=('Arial', 10), padding=5)\n",
    "    style.configure('TCombobox', font=('Arial', 10), padding=5)\n",
    "    style.map('TCombobox', fieldbackground=[('readonly', 'white')],\n",
    "              selectbackground=[('readonly', 'white')],\n",
    "              selectforeground=[('readonly', 'black')])\n",
    "    style.configure('TFrame', background='white')  # 프레임 배경색 설정\n",
    "    style.configure('Horizontal.TProgressbar', background='#FA8072')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bf8be45f-60f6-424c-9618-7cdd39c4a702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path not found in the JSON structure: 'Key not found in the JSON structure.'\n",
      "Path not found in the JSON structure: 'Key not found in the JSON structure.'\n",
      "Path not found in the JSON structure: 'Key not found in the JSON structure.'\n",
      "콘코던스 키 입력하지 않음\n",
      "{'C:/Users/pc021/Desktop/JASON\\\\WBRW1900000002.csv':                                               Sentence\n",
      "0                                                 워낭소리\n",
      "1    다큐멘터리 영화 <워낭소리>를 보았다. 관객 수가 이백만을 넘었다고 한다. 영화에 ...\n",
      "2    열네 살 소년은 외양간 모퉁이에서 목욕하는 중이었다. 둥근 나무통 안에 발가벗고 앉...\n",
      "3    목욕을 끝내고 외양간을 나오면서 소년은 소를 걷어차려고 하다가 멈추었다. 뭔가 모를...\n",
      "4    중학교 입학에 실패한 후 소년은 부모님의 농사일을 도우며 그럭저럭 지냈다. 소년의 ...\n",
      "..                                                 ...\n",
      "402  지난 3월 학기 초였다. 교양과목 수업에 들어갔는데, 나이 든 여성 세 사람이 눈에...\n",
      "403  놀라운 여성을 만나는 것은 학교뿐만 아니다. 우리 사회 곳곳에 열성적인 주부들이 자...\n",
      "404  반야월에 있는 ‘아띠 도서관’의 경우를 보자. 지역주민이 만든 작은 도서관이다. 주...\n",
      "405  결혼한 주부 여성을 ‘아줌마’라고 호칭한다. 아주머니를 낮추어 부르는 말이다. ‘못...\n",
      "406  아줌마가 움직이면 동네가 변할 정도로 그 파워가 엄청난 것이 사실이다. 아줌마는 그...\n",
      "\n",
      "[407 rows x 1 columns], 'C:/Users/pc021/Desktop/JASON\\\\WCRW1900000001.csv':                                               Sentence\n",
      "0                                            ‘다람쥐’의 어원\n",
      "1    ‘다람쥐’는 ‘다람쥐 쳇바퀴 돌듯’, ‘다람쥐 밤 까먹듯’과 같은 속담이나 ‘산골짝...\n",
      "2    ‘다람쥐’가 ‘다람’과 ‘쥐’로 분석된다는 사실은 쉽게 알 수 있다. ‘다람쥐’의 ...\n",
      "3    ‘다람쥐’는 ‘ᄃᆞᄅᆞᆷ쥐’라는 형태로 18세기에 처음 등장한다. 그리고 ‘다ᄅᆞᆷ...\n",
      "4    쥐(豆鼠) <한청문감>(18세기) 다ᄅᆞᆷ쥐 鼯 <한불자전(1880년)> 하하 탐음...\n",
      "..                                                 ...\n",
      "127                       6’) 소비자보호원은 사고 발생 가능성이 높다며……\n",
      "128                                   7’) 중앙노동위원회에서는……\n",
      "129                        물론 억지로 자주 사용해서 익숙해진 용어도 있다.\n",
      "130  경실련(경제정의실천시민연합), 민예총(한국민족예술인총연합), 헌재(헌법재판소), 행...\n",
      "131  신문과 방송이 모두가 이해하겠지 하는 마음에 원칙 없이 줄인 말들이 사람들의 입에 ...\n",
      "\n",
      "[132 rows x 1 columns], 'C:/Users/pc021/Desktop/JASON\\\\WCRW1900000003.csv':                                             Sentence\n",
      "0                                              화면음악실\n",
      "1  여름이 한창이다. 한강 둔치에는 공휴일이면 더위를 식히러 많은 시민들이 모여들고 있...\n",
      "2      이번 호에도 지난 호에 이어 북한 사전에 실리지 않은 말들을 소개해 보기로 한다.\n",
      "3  2중영예붉은기는 스마트폰 북한에서 ‘3대 혁명 붉은기를 두 번 수여 받은 학교’이다...\n",
      "4  개개움은 ‘부대끼는 일’의 뜻이다. “자신의 역스러운 이야기를 꺼내고싶지 않아 그적...\n",
      "5  치닥질은 ‘성가신 일’의 뜻이다. “≪하지만 기생을 끼고내려와 배를 부르는데야 어찌...\n",
      "6  평양열풍은 북한에서 김정일 스마트폰을 위원장을 바람에 비유하여 이르는 말이다.“ 시...\n",
      "7  화면음악실은 ‘화면 음악을 만드는 곳’이다. 북한에서 화면 음악은 ‘영화의 해당 장...\n",
      "8  요즈음 사회 각 방면에서 남북 교류가 스마트폰과 활발하게 진행되고 있다. 서로 간의...}\n",
      "{'C:/Users/pc021/Desktop/JASON\\\\WBRW1900000002.csv':                                               Sentence\n",
      "0                                                 워낭소리\n",
      "1    다큐멘터리 영화 <워낭소리>를 보았다. 관객 수가 이백만을 넘었다고 한다. 영화에 ...\n",
      "2    열네 살 소년은 외양간 모퉁이에서 목욕하는 중이었다. 둥근 나무통 안에 발가벗고 앉...\n",
      "3    목욕을 끝내고 외양간을 나오면서 소년은 소를 걷어차려고 하다가 멈추었다. 뭔가 모를...\n",
      "4    중학교 입학에 실패한 후 소년은 부모님의 농사일을 도우며 그럭저럭 지냈다. 소년의 ...\n",
      "..                                                 ...\n",
      "402  지난 3월 학기 초였다. 교양과목 수업에 들어갔는데, 나이 든 여성 세 사람이 눈에...\n",
      "403  놀라운 여성을 만나는 것은 학교뿐만 아니다. 우리 사회 곳곳에 열성적인 주부들이 자...\n",
      "404  반야월에 있는 ‘아띠 도서관’의 경우를 보자. 지역주민이 만든 작은 도서관이다. 주...\n",
      "405  결혼한 주부 여성을 ‘아줌마’라고 호칭한다. 아주머니를 낮추어 부르는 말이다. ‘못...\n",
      "406  아줌마가 움직이면 동네가 변할 정도로 그 파워가 엄청난 것이 사실이다. 아줌마는 그...\n",
      "\n",
      "[407 rows x 1 columns], 'C:/Users/pc021/Desktop/JASON\\\\WCRW1900000001.csv':                                               Sentence\n",
      "0                                            ‘다람쥐’의 어원\n",
      "1    ‘다람쥐’는 ‘다람쥐 쳇바퀴 돌듯’, ‘다람쥐 밤 까먹듯’과 같은 속담이나 ‘산골짝...\n",
      "2    ‘다람쥐’가 ‘다람’과 ‘쥐’로 분석된다는 사실은 쉽게 알 수 있다. ‘다람쥐’의 ...\n",
      "3    ‘다람쥐’는 ‘ᄃᆞᄅᆞᆷ쥐’라는 형태로 18세기에 처음 등장한다. 그리고 ‘다ᄅᆞᆷ...\n",
      "4    쥐(豆鼠) <한청문감>(18세기) 다ᄅᆞᆷ쥐 鼯 <한불자전(1880년)> 하하 탐음...\n",
      "..                                                 ...\n",
      "127                       6’) 소비자보호원은 사고 발생 가능성이 높다며……\n",
      "128                                   7’) 중앙노동위원회에서는……\n",
      "129                        물론 억지로 자주 사용해서 익숙해진 용어도 있다.\n",
      "130  경실련(경제정의실천시민연합), 민예총(한국민족예술인총연합), 헌재(헌법재판소), 행...\n",
      "131  신문과 방송이 모두가 이해하겠지 하는 마음에 원칙 없이 줄인 말들이 사람들의 입에 ...\n",
      "\n",
      "[132 rows x 1 columns], 'C:/Users/pc021/Desktop/JASON\\\\WCRW1900000003.csv':                                             Sentence\n",
      "0                                              화면음악실\n",
      "1  여름이 한창이다. 한강 둔치에는 공휴일이면 더위를 식히러 많은 시민들이 모여들고 있...\n",
      "2      이번 호에도 지난 호에 이어 북한 사전에 실리지 않은 말들을 소개해 보기로 한다.\n",
      "3  2중영예붉은기는 스마트폰 북한에서 ‘3대 혁명 붉은기를 두 번 수여 받은 학교’이다...\n",
      "4  개개움은 ‘부대끼는 일’의 뜻이다. “자신의 역스러운 이야기를 꺼내고싶지 않아 그적...\n",
      "5  치닥질은 ‘성가신 일’의 뜻이다. “≪하지만 기생을 끼고내려와 배를 부르는데야 어찌...\n",
      "6  평양열풍은 북한에서 김정일 스마트폰을 위원장을 바람에 비유하여 이르는 말이다.“ 시...\n",
      "7  화면음악실은 ‘화면 음악을 만드는 곳’이다. 북한에서 화면 음악은 ‘영화의 해당 장...\n",
      "8  요즈음 사회 각 방면에서 남북 교류가 스마트폰과 활발하게 진행되고 있다. 서로 간의...}\n"
     ]
    }
   ],
   "source": [
    "# Action: GUI 생성\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"n-gram 및 형태소 분석기 v1.1\")\n",
    "# 프로그램의 고정된 크기\n",
    "program_width = 670\n",
    "program_height = 850\n",
    "\n",
    "# 화면의 중앙에 프로그램이 위치하도록 좌표를 계산합니다.\n",
    "screen_width = root.winfo_screenwidth()\n",
    "screen_height = root.winfo_screenheight()\n",
    "center_x = int((screen_width - program_width) / 2)\n",
    "center_y = int((screen_height - program_height) / 2)\n",
    "\n",
    "# 프로그램의 위치와 크기를 설정합니다.\n",
    "root.geometry(f'{program_width}x{program_height}+{center_x}+{center_y}')\n",
    "\n",
    "configure_styles()\n",
    "\n",
    "# 숫자 입력 확인을 위한 유효성 검사 커맨드 생성\n",
    "vcmd = root.register(only_numbers)\n",
    "\n",
    "# (1,1)\n",
    "tag_label = tk.Label(root, text=\"* 태그\", anchor='w')\n",
    "tag_label.grid(row=0, column=0, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (1,2)\n",
    "user_input = ttk.Entry(root)\n",
    "user_input.grid(row=0, column=1, sticky='ew', padx=10, pady=5)\n",
    "\n",
    "# (1,3)\n",
    "tag_menu_label = tk.Label(root, text=\"* 태그 선택\", anchor='w')\n",
    "tag_menu_label.grid(row=0, column=2, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (1,4)\n",
    "tag_variable = StringVar(root)\n",
    "tag_variable.trace(\"w\", update_user_input)\n",
    "tag_options = {\n",
    "    \"신문 말뭉치\": \"document.paragraph.form\",\n",
    "    \"일상 대화 말뭉치\": \"document.utterance.form\",\n",
    "    \"직접 입력\": \"\",\n",
    "}\n",
    "tag_menu = ttk.Combobox(root, textvariable=tag_variable, values=list(tag_options.keys()), state='readonly')\n",
    "tag_menu.grid(row=0, column=3, sticky='ew', padx=10, pady=5)\n",
    "tag_menu.set(\"직접 입력\")\n",
    "\n",
    "\n",
    "# (4,1)\n",
    "filter_key_label = tk.Label(root, text=\"필터키\", anchor='w')\n",
    "filter_key_label.grid(row=1, column=0, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (4,2)\n",
    "filter_key_entry = ttk.Entry(root)\n",
    "filter_key_entry.grid(row=1, column=1, sticky='ew', padx=10, pady=5)\n",
    "\n",
    "# (4,3)\n",
    "tag_menu_label = tk.Label(root, text=\"필터값\", anchor='w')\n",
    "tag_menu_label.grid(row=1, column=2, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (4,4)\n",
    "filter_value_entry = ttk.Entry(root)\n",
    "filter_value_entry.grid(row=1, column=3, sticky='ew', padx=10, pady=5)\n",
    "\n",
    "# (5,1)\n",
    "concordance_label = tk.Label(root, text=\"콘코던스 단어(|로 구분)\", anchor='w')\n",
    "concordance_label.grid(row=2, column=0, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (5,2)\n",
    "concordance_entry = ttk.Entry(root)\n",
    "concordance_entry.grid(row=2, column=1, sticky='ew', padx=10, pady=5)\n",
    "\n",
    "# (5,3)\n",
    "exclude_words_label = tk.Label(root, text=\"제외 단어(|로 구분)\", anchor='w')\n",
    "exclude_words_label.grid(row=2, column=2, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (5,4)\n",
    "exclude_words_entry = ttk.Entry(root)\n",
    "exclude_words_entry.grid(row=2, column=3, sticky='ew', padx=10, pady=5)\n",
    "\n",
    "ngram_cnt_label = tk.Label(root, text=\"* n-gram 사이즈\", anchor='w')\n",
    "ngram_cnt_label.grid(row=3, column=0, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (2,2)\n",
    "ngram_cnt_entry = ttk.Entry(root, validate=\"key\", validatecommand=(vcmd, '%S'))\n",
    "ngram_cnt_entry.grid(row=3, column=1, sticky='ew', padx=10, pady=5)\n",
    "ngram_cnt_entry.insert(0, \"0\")\n",
    "\n",
    "# (7,1)\n",
    "folder_button = ttk.Button(root, text=\"폴더 선택 및 분석\", command=select_final_language_analyzer)\n",
    "folder_button.grid(row=5, column=0, sticky='ew', padx=10, pady=5, columnspan=4)\n",
    "\n",
    "# (2,3)\n",
    "tag_menu_label = tk.Label(root, text=\"* 형태소 분석기\", anchor='w')\n",
    "tag_menu_label.grid(row=3, column=2, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (2,4)\n",
    "morpheme_analyzer = StringVar(root)\n",
    "morpheme_menu = ttk.Combobox(root, textvariable=morpheme_analyzer, values=list(morpheme_analyzers.keys()), state='readonly')\n",
    "morpheme_menu.grid(row=3, column=3, sticky='ew', padx=10, pady=5)\n",
    "morpheme_menu.set(\"선택 없음\")\n",
    "\n",
    "window_cnt_label = tk.Label(root, text=\"* window 사이즈\", anchor='w')\n",
    "window_cnt_label.grid(row=4, column=0, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (2,2)\n",
    "window_cnt_entry = ttk.Entry(root, validate=\"key\", validatecommand=(vcmd, '%S'))\n",
    "window_cnt_entry.grid(row=4, column=1, sticky='ew', padx=10, pady=5)\n",
    "window_cnt_entry.insert(0, \"0\")\n",
    "\n",
    "# (6,1)\n",
    "linguistic_unit_label = tk.Label(root, text=\"언어 단위 선택\", anchor='w')\n",
    "linguistic_unit_label.grid(row=4, column=2, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (6,2)\n",
    "linguistic_unit_variable = StringVar(root)\n",
    "linguistic_unit_variable.trace(\"w\", update_user_input)\n",
    "linguistic_unit_options = {\n",
    "    \"형태소\": \"\",\n",
    "    \"어절\": \"\"}\n",
    "linguistic_unit_menu = ttk.Combobox(root, textvariable=linguistic_unit_variable, values=list(linguistic_unit_options.keys()), state='readonly')\n",
    "linguistic_unit_menu.grid(row=4, column=3, sticky='ew', padx=10, pady=5)\n",
    "linguistic_unit_menu.set(\"직접 입력\")\n",
    "\n",
    "\n",
    "root.grid_columnconfigure(1, weight=1)\n",
    "root.grid_rowconfigure(6, weight=1)\n",
    "root.grid_rowconfigure(7, weight=1)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1bcb1bf5-6413-40aa-bc01-27f5631e9161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>이/VCP 다/EF ./SF</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>었/EP 다/EF ./SF</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>것/NNB 이/VCP 다/EF</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>있/VX 다/EF ./SF</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>았/EP 다/EF ./SF</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43133</th>\n",
       "      <td>이/JKS 사회/NNG 문제/NNG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43134</th>\n",
       "      <td>사회/NNG 문제/NNG 가/JKS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43135</th>\n",
       "      <td>문제/NNG 가/JKS 된다는/VV+ETM</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43136</th>\n",
       "      <td>가/JKS 된다는/VV+ETM 사회학/NNG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43137</th>\n",
       "      <td>무한/NNG 하/XSA 다/EF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43138 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Word  Frequency\n",
       "0               이/VCP 다/EF ./SF        513\n",
       "1                었/EP 다/EF ./SF        189\n",
       "2              것/NNB 이/VCP 다/EF        126\n",
       "3                있/VX 다/EF ./SF         87\n",
       "4                았/EP 다/EF ./SF         86\n",
       "...                         ...        ...\n",
       "43133       이/JKS 사회/NNG 문제/NNG          1\n",
       "43134       사회/NNG 문제/NNG 가/JKS          1\n",
       "43135   문제/NNG 가/JKS 된다는/VV+ETM          1\n",
       "43136  가/JKS 된다는/VV+ETM 사회학/NNG          1\n",
       "43137         무한/NNG 하/XSA 다/EF          1\n",
       "\n",
       "[43138 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(r\"C:\\Users\\pc021\\Desktop\\JASON\\WBRW1900000002.csv_count.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_38",
   "language": "python",
   "name": "python_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
