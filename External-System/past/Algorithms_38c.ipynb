{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "581356af-9fbd-4527-bb5b-a8b118671073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import ijson\n",
    "import csv\n",
    "import tkinter as tk\n",
    "import pandas as pd\n",
    "import chardet\n",
    "from openpyxl import Workbook \n",
    "\n",
    "# Natural Language processing 22 years old\n",
    "from collections import Counter \n",
    "from konlpy.tag import Okt, Komoran, Hannanum, Kkma, Mecab\n",
    "\n",
    "# Visualize \n",
    "from tkinter import filedialog, messagebox, ttk, StringVar\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "from matplotlib import font_manager, rc\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cd2db66-48cf-4cd5-8e08-680a24b38bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 형태소 분석기 설정 \n",
    "\n",
    "# 1. 단어 통계를 위한 Counter 객체 생성\n",
    "word_counter = Counter()\n",
    "\n",
    "# 2. 형태소 분석기(Mecab 로드)\n",
    "mecab = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "\n",
    "# 3. 형태소 분석기 초기화\n",
    "morpheme_analyzers = {\n",
    "    \"선택 없음\": None,\n",
    "    \"Okt\": Okt(),\n",
    "    \"Komoran\": Komoran(),\n",
    "    \"Hannanum\": Hannanum(),\n",
    "    \"Kkma\": Kkma(),\n",
    "    'Mecab': mecab }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02d4a695-fffa-4208-93e2-6759d72d1208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 폰트 설정\n",
    "\n",
    "#if getattr(sys, 'frozen', False):  # 코드가 PyInstaller로 패키징된 경우\n",
    "#    base_path = sys._MEIPASS\n",
    "#else:\n",
    "#    base_path = os.path.dirname(__file__)\n",
    "\n",
    "base_path = os.getcwd()\n",
    "\n",
    "font_path = os.path.join(base_path, \"fonts\", \"malgun.ttf\")\n",
    "font_name = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c5b6656-f5cf-42d8-911c-92f5be525672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: ngrams 함수 생성\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    # Input: s = string, n = size of the ngram\n",
    "    # Output: list of ngrams\n",
    "    tokens = s.split()\n",
    "    \n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)]) \n",
    "    \n",
    "    n_grams_dataset = pd.DataFrame([\" \".join(ngram) for ngram in ngrams],columns=['n_grams'])\n",
    "\n",
    "    return n_grams_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac7634d1-530d-406b-b917-a4a9c08fdef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_generate_ngrams(sentences,n):\n",
    "    ngrams_list = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        #print(sentence)\n",
    "        n_grams_sentence = generate_ngrams(sentence,n)\n",
    "        ngrams_list.append(n_grams_sentence)\n",
    "\n",
    "    n_gram_dataset = pd.concat(ngrams_list).reset_index().dropna()\n",
    "    return n_gram_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "66f45b0d-63b9-45a2-8d19-d3c1e705f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 콘코던스 단어 추적 함수 구현\n",
    "\n",
    "def track_down_concordance_words():\n",
    "    \n",
    "    # concordance_entry에서 단어 목록을 가져옵니다.\n",
    "    #concordance_words = concordance_entry.split('|')\n",
    "    concordance_words_get = concordance_entry.get()\n",
    "    concordance_words_get = replace_strip(concordance_words_get)\n",
    "    \n",
    "    if concordance_words_get != 'None':\n",
    "        concordance_words = concordance_words_get.split('|')\n",
    "        concordance_words = [word.strip() for word in concordance_words]\n",
    "\n",
    "        return  concordance_words\n",
    "\n",
    "    else:\n",
    "        return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8defdcf3-68d2-4650-8dc1-eedcbb11ad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 제외 단어 추적 함수 구현\n",
    "\n",
    "def track_down_exclude_words():\n",
    "\n",
    "    # 제외 단어 목록을 가져옵니다.\n",
    "    exclude_words = exclude_words_entry.get().split('|')\n",
    "    #exclude_words = exclude_words_entry.split('|')\n",
    "    exclude_words = [word.strip() for word in exclude_words]\n",
    "\n",
    "    return exclude_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a7f3a413-d094-42ab-ab06-dc680444997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_strip(concordance_words_get):\n",
    "    if concordance_words_get.strip() == '':\n",
    "        return 'None'\n",
    "    else:\n",
    "        return concordance_words_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fedac186-d344-42b2-aff3-d58689a52d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 콘코던스 필터링 함수 구현\n",
    "\n",
    "def execute_concordence_sentence_only_fuction():\n",
    "    # 콘코던스 함수 실행\n",
    "    concordance_words_get = track_down_concordance_words()\n",
    "    # 문장 추출 함수 사용\n",
    "    analyzed_folder,folder_path= extract_materials_be_analyzed() \n",
    "    # 폴더 경로 설정\n",
    "    #folder_path = filedialog.askdirectory()\n",
    "\n",
    "    concordence_dict = {}\n",
    "    \n",
    "    if (concordance_words_get) and ('None' not in concordance_words_get):\n",
    "        #dict_with_concordance={'original': [], 'analyzed': []}\n",
    "        list_with_concordence = [] \n",
    "        for key,value in analyzed_folder.items():\n",
    "            all_sentences = value['Sentence']\n",
    "            if any(any(con_word in sentence for con_word in concordance_words_get) for sentence in all_sentences):\n",
    "                print(\"포함된 문장이 존재함\")\n",
    "                for con_word in concordance_words_get:\n",
    "                    for sentence in all_sentences:\n",
    "                        if con_word in sentence:\n",
    "                            list_with_concordence.append(sentence)\n",
    "\n",
    "                # 콘코던스 필러링 데이터 프레임화\n",
    "                df_list_with_concordence = pd.DataFrame(list_with_concordence,columns=['Sentence'])\n",
    "                # 콘코던스 딕셔너리 생성\n",
    "                concordence_dict[key] = df_list_with_concordence \n",
    "            else:\n",
    "                print(\"포함된 문장이 없습니다.\")\n",
    "\n",
    "        return  concordence_dict,folder_path\n",
    "    else:\n",
    "        #print(\"콘코던스 키 입력하지 않음\")\n",
    "        #print(analyzed_folder)\n",
    "        return analyzed_folder,folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c21e4c66-0332-4878-815a-265b1b36bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 문장 품사 태깅함수 구현\n",
    "\n",
    "def tag_part_sentence():\n",
    "\n",
    "    # 분석 딕셔너리 \n",
    "    sentence_dicts,folder_path = execute_concordence_sentence_only_fuction() \n",
    "\n",
    "    # 제외 단어 추척 함수 사용\n",
    "    exclude_words_entry = track_down_exclude_words()\n",
    "    \n",
    "    # n_grams 생성 \n",
    "    n_gram = ngram_cnt_entry.get()\n",
    "\n",
    "    # 표 표출수 생성\n",
    "    if extract_cnt_entry!=0:\n",
    "        table_limit_count = int(extract_cnt_entry.get())\n",
    "\n",
    "    # 형태소 분석기 생성\n",
    "    morpheme_menu_get = morpheme_menu.get()\n",
    "\n",
    "    \n",
    "    # 아웃풋 딕셔너리 생성\n",
    "    all_sentences = {'original': [], 'analyzed': []}\n",
    "    \n",
    "    for key,value in sentence_dicts.items():\n",
    "\n",
    "        # 데이터 프레임 문장 추출\n",
    "        sentences = value['Sentence']\n",
    "        \n",
    "        # 제외 단어 추척 함수 사용\n",
    "        #exclude_words = track_down_exclude_words('먹었다')\n",
    "    \n",
    "        for sentence in sentences:\n",
    "            # 선택한 형태소 분석기로 문장을 형태소 분석합니다.\n",
    "            morphemes = morpheme_analyzers[morpheme_menu_get].pos(sentence)\n",
    "\n",
    "            # 제외 단어 목록에 포함되지 않은 형태소만 추가합니다.\n",
    "            filtered_morphemes = [f\"{word}/{tag}\" for word, tag in morphemes if word not in exclude_words_entry]\n",
    "\n",
    "            # 문장을 형태소 분석된 형태로 변환합니다.\n",
    "            analyzed_sentence = ' '.join(filtered_morphemes)\n",
    "\n",
    "            # 기존 문장 저장\n",
    "            all_sentences['original'].append(sentence)\n",
    "    \n",
    "            # 분석된 문장 저장\n",
    "            all_sentences['analyzed'].append(analyzed_sentence)\n",
    "\n",
    "        # n-gram을 생성합니다.\n",
    "        #ngrams = apply_generate_ngrams(all_sentences['analyzed'], int(n_gram))\n",
    "\n",
    "        # n_counts 생성\n",
    "        #ngrams_count = count_n_grams(ngrams)\n",
    "\n",
    "        # 분석 데이터 프레임 저장\n",
    "        #pd.DataFrame(all_sentences).to_csv(f'{key}', index=False)\n",
    "        #ngrams_count.to_csv(f'{key}_count.csv', index=False)\n",
    "\n",
    "    # n-gram을 생성합니다.\n",
    "    ngrams = apply_generate_ngrams(all_sentences['analyzed'], int(n_gram))\n",
    "\n",
    "    # n_counts 생성\n",
    "    ngrams_count = count_n_grams(ngrams)\n",
    "\n",
    "    if table_limit_count!=0:\n",
    "        ngrams_count = ngrams_count[:table_limit_count]\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    pd.DataFrame(all_sentences).to_csv(f'{folder_path}\\\\analysis_table.csv' ,index=False)\n",
    "    ngrams_count.to_csv(f'{folder_path}\\\\count.csv', index=False)\n",
    "    \n",
    "    # 시각화 함수\n",
    "    plot_data(ngrams_count)\n",
    "    \n",
    "    return pd.DataFrame(all_sentences),folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0ed8294c-af7c-421f-9de4-93c123245391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_sentence_to_phrase():\n",
    "    # 분석 딕셔너리 \n",
    "    sentence_dicts,folder_path = execute_concordence_sentence_only_fuction() \n",
    "    \n",
    "    # 제외 단어 추척 함수 사용\n",
    "    exclude_words_entry = track_down_exclude_words()\n",
    "    \n",
    "    # n_grams 생성 \n",
    "    n_gram = ngram_cnt_entry.get()\n",
    "\n",
    "    # 표 표출수 생성\n",
    "    if extract_cnt_entry!=0:\n",
    "        table_limit_count = int(extract_cnt_entry.get())\n",
    "\n",
    "    # 아웃풋 딕셔너리 생성\n",
    "    all_sentences = {'original': [], 'analyzed': []}\n",
    "    \n",
    "    for key,value in sentence_dicts.items():\n",
    "   \n",
    "        # 데이터 프레임 문장 추출\n",
    "        sentences = value['Sentence']\n",
    "    \n",
    "        for sentence in sentences:\n",
    "            # 선택한 형태소 분석기로 문장을 형태소 분석합니다.\n",
    "            morphemes = sentence.split(' ')\n",
    "            # 제외 단어 목록에 포함되지 않은 형태소만 추가합니다.\n",
    "            filtered_morphemes = [f\"{word}\" for word in morphemes if word not in exclude_words_entry]\n",
    "\n",
    "            # 문장을 형태소 분석된 형태로 변환합니다.\n",
    "            analyzed_sentence = ' '.join(filtered_morphemes)\n",
    "\n",
    "            # 기존 문장 저장\n",
    "            all_sentences['original'].append(sentence)\n",
    "    \n",
    "            # 분석된 문장 저장\n",
    "            all_sentences['analyzed'].append(analyzed_sentence)\n",
    "\n",
    "          \n",
    "    # n-gram을 생성합니다.\n",
    "    ngrams = apply_generate_ngrams(all_sentences['analyzed'], int(n_gram))\n",
    "\n",
    "    # n_counts 생성\n",
    "    ngrams_count = count_n_grams(ngrams)\n",
    "\n",
    "    if table_limit_count!=0:\n",
    "        ngrams_count = ngrams_count[:table_limit_count]\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    # 분석 데이터 프레임 저장\n",
    "    #pd.DataFrame(all_sentences).to_csv(f'{key}', index=False)\n",
    "    #ngrams_count.to_csv(f'{key}_count.csv', index=False)\n",
    "        \n",
    "\n",
    "    pd.DataFrame(all_sentences).to_csv(f'{folder_path}\\\\analysis_table.csv' ,index=False)\n",
    "    ngrams_count.to_csv(f'{folder_path}\\\\count.csv', index=False)\n",
    "    \n",
    "    # 시각화 함수\n",
    "    plot_data(ngrams_count)\n",
    "    \n",
    "    return pd.DataFrame(all_sentences),folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4e4dd481-27f1-4173-86ad-565beadda602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(words):\n",
    "    # 단어 카운트\n",
    "    word_count = Counter(words['n_grams'])\n",
    "    \n",
    "    word_count_data = pd.DataFrame(list(word_count.items()), columns=['Word','Frequency']).dropna()\n",
    "\n",
    "    word_count_data = word_count_data.sort_values(by='Frequency', ascending=False)\n",
    "    \n",
    "    return word_count_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bb19008c-e7c2-4f25-b59c-07142375f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 제이슨 필터링 함수 구현\n",
    "\n",
    "def filter_jason_folder(folder_path,filter_key,expected_value):\n",
    "    \"\"\" 필터 값에 맞는 JASON 데이터 추출하기 \n",
    "\n",
    "    파라미터: 폴더 경로, 필터 키, 필터 값\n",
    "\n",
    "    반환 값: 필터 값에 맞는 JASON을 추가한 폴더 \n",
    "    \"\"\"\n",
    "    # 제이슨 폴더 생성\n",
    "    jason_folder = []\n",
    "    filter_jason_folder = []\n",
    "\n",
    "    #print(f\"현재 폴더:{folder_path}\") \n",
    "    # 초기 값 설정\n",
    "    cheked_file_path = None \n",
    "    #print(\"현재 폴더:\",folder_path)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if (filename.endswith('.json')) | (filename.endswith('.JSON')):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            jason_folder.append(file_path)\n",
    "            #print(f\"필터링 폴더:{jason_folder}\")\n",
    "            #파일 인코딩 체크\n",
    "            file_encoding = detect_encoding(file_path)\n",
    "            with open(file_path, 'r', encoding=file_encoding) as file:\n",
    "                data = json.load(file)\n",
    "                # 필터 키 존재한다면\n",
    "                if filter_key != None:\n",
    "                    cheked_file_path = inspect_jason(data,filter_key,expected_value,file_path)\n",
    "                # 만약 조건에 맞는 jason 파일이 있다면\n",
    "                if cheked_file_path:\n",
    "                    filter_jason_folder.append(cheked_file_path)\n",
    "                    \n",
    "    # 필터링 제이슨 폴터가 존재한다면            \n",
    "    if filter_jason_folder:\n",
    "        #print(\"필터링 조건 통과\")\n",
    "        return filter_jason_folder\n",
    "    else:\n",
    "        #print(\"조건 통과하지 않음\")\n",
    "        return jason_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "513dbf30-9b9c-4b51-8c37-9f55c7743ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: filter key, values를 이용한 jason 파일 필터링 \n",
    "\n",
    "def inspect_jason(jason_data,filter_key,expected_value,file_path):\n",
    "    \"\"\"\n",
    "    필터 값과 일치하는 JASON 데이터 검사\n",
    "\n",
    "    파라미터: 데이터, 필터 키, 필터 값, 데이터 경로\n",
    "\n",
    "    반환 값: 필터 값에 맞는 JASON 데이터 경로\n",
    "    \"\"\"\n",
    "    # 필터 키 분리\n",
    "    filter_key_list = filter_key.split('.')\n",
    "    \n",
    "    # 조건 실행\n",
    "    try:\n",
    "        for key in filter_key_list:\n",
    "            if isinstance(jason_data,dict) and key in jason_data:\n",
    "                jason_data = jason_data[key]\n",
    "            elif isinstance(jason_data,list):\n",
    "                 # 리스트의 경우, 리스트의 모든 요소를 포함하는 새 리스트를 생성\n",
    "                jason_data = [subvalue[key] for subvalue in jason_data if key in subvalue]\n",
    "            else:\n",
    "                raise KeyError(\"Key not found in the JSON structure.\")\n",
    "        # 최종 값을 확인\n",
    "        if isinstance(jason_data, list):\n",
    "            matches = [val for val in jason_data if expected_value in val]\n",
    "            if matches:\n",
    "                print(f\"Match found: {matches}\")\n",
    "                return file_path\n",
    "            else:\n",
    "                print(\"No match found.\")\n",
    "        else:\n",
    "            if expected_value in jason_data:\n",
    "                return file_path\n",
    "                print(f\"Match found: {jason_data}\")\n",
    "            else:\n",
    "                print(\"No match found.\")                \n",
    "\n",
    "    except KeyError as e:\n",
    "            print(f\"Path not found in the JSON structure: {e}\")\n",
    "    except Exception as e:\n",
    "            print(f\"An error occurred: {e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "497a31a9-bcf2-4f04-8ce7-6407337ec41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 리스트 틀 정제 함수 \n",
    "\n",
    "def flatten_list(nested_list):\n",
    "    flat_list = []\n",
    "    for element in nested_list:\n",
    "        if isinstance(element, list):  # 요소가 리스트인 경우, 재귀 호출\n",
    "            flat_list.extend(flatten_list(element))\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4e827da6-cb28-41d3-be52-a32b823ae10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: Tag 내용 추출 함수 구현\n",
    "\n",
    "def extract_tag(data,path_elements):\n",
    "    \"\"\" JASON 데이터의 TAG 내용 추출\n",
    "    \n",
    "    파라미터: JASN 데이터, 테그 리스트\n",
    "\n",
    "    반환 값: 테그 분석 내용    \n",
    "    \"\"\"\n",
    "    # tag 원소를 담을 그릇\n",
    "    tag_bowl = []\n",
    "    last_element = path_elements[-1]\n",
    "    \n",
    "    try:\n",
    "        # 첫 번째 경로 요소를 추출\n",
    "        first_element = path_elements[0]\n",
    "        # 현재 경로 요소가 리스트를 요구하는 경우\n",
    "        if isinstance(data, list):\n",
    "            if (first_element==last_element):\n",
    "                for num in range(len(data)):\n",
    "                    val = data[num][first_element]\n",
    "                    tag_bowl.append(val)             \n",
    "                return tag_bowl\n",
    "            else:      \n",
    "                # 리스트의 각 요소에 대해 재귀적으로 함수를 호출\n",
    "                result = [extract_tag(item, path_elements) for item in data]\n",
    "                # None 값을 제외한 결과만 필터링\n",
    "                return [item for item in result if item is not None]       \n",
    "        # 현재 데이터가 딕셔너리이고 경로 요소가 키로 존재하는 경우\n",
    "        elif isinstance(data, dict) and first_element in data:      \n",
    "            # 다음 경로 요소로 재귀적으로 함수를 호출    \n",
    "            return extract_tag(data[first_element], path_elements[1:])      \n",
    "        elif (first_element==last_element):\n",
    "            return data[first_element]           \n",
    "    except KeyError as e:\n",
    "            print(f\"Path not found in the JSON structure: {e}\")\n",
    "    except Exception as e:\n",
    "            print(f\"An error occurred: {e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2f6e8aa4-57aa-48b0-a134-d856dd3641af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 폴더 필터링, 분석 테그 내용 컴바인 함수 구현 \n",
    "\n",
    "def extract_materials_be_analyzed():\n",
    "    \"\"\" 폴더 필터링, 분석 태그 내용 컴바인 함수 구현\n",
    "\n",
    "    반환 값: 테그 내용 리스트\n",
    "    \"\"\"\n",
    "    # 태그 설정\n",
    "    path_elements = user_input.get().split('.')\n",
    "    \n",
    "    if path_elements==[\"\"]:\n",
    "        messagebox.showinfo(\"Error\",\"태그 값을 입력하세요\")\n",
    "        return   \n",
    "    \n",
    "    # 폴더 경로 설정\n",
    "    folder_path = filedialog.askdirectory()\n",
    "    #print(\"현자 경로:\",folder_path)\n",
    "    \n",
    "    # 필터 키 설정\n",
    "    filter_key = filter_key_entry.get()\n",
    "\n",
    "    # 필터 값 설정\n",
    "    tag_name = filter_value_entry.get()\n",
    "    \n",
    "    # 폴더에서 필터한 폴더를 반환\n",
    "    fited_jason_forder = filter_jason_folder(folder_path,filter_key,tag_name)\n",
    "\n",
    "    # 분석할 내용을 담을 딕셔너리 생성 \n",
    "    analysis_bowl = {}\n",
    "\n",
    "    # 폴더를 순회하면서 분석 내용 추출\n",
    "    for jason_file in fited_jason_forder:\n",
    "\n",
    "        # 파일 인코딩 체크\n",
    "        file_encoding = detect_encoding(jason_file)\n",
    "        \n",
    "        with open(jason_file, 'r', encoding=file_encoding) as file:\n",
    "            # 해당 데이터 \n",
    "            data = json.load(file)\n",
    "            # 파일 이름 생성 \n",
    "            sentence_csv_path = create_file_name(folder_path,jason_file)\n",
    "             # 내용 추출 \n",
    "            rows_bowl = extract_tag(data,path_elements) \n",
    "            # 리스트 정제         \n",
    "            clean_list = flatten_list(rows_bowl)\n",
    "             # 데이터 프레임화\n",
    "            clean_list = pd.DataFrame(clean_list,columns=['Sentence'])\n",
    "            # 데이터 딕셔너리 추가\n",
    "            analysis_bowl[sentence_csv_path] = clean_list\n",
    "\n",
    "    return analysis_bowl,folder_path       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0213fc18-af90-47f2-9a9b-5234875962f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 파일 이름 생성 함수\n",
    "\n",
    "def create_file_name(folder_path,jason_file):\n",
    "    \n",
    "    # 파일 이름만 추출\n",
    "    file_name_with_extension = os.path.basename(jason_file)\n",
    "\n",
    "    # 확장자 제거\n",
    "    file_name, _ = os.path.splitext(file_name_with_extension)\n",
    "    \n",
    "    sentence_csv_path = os.path.join(folder_path, f\"{file_name}.csv\")\n",
    "\n",
    "    return sentence_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3f0dbacf-46c1-4245-88c3-e93c4c162fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: Jason 인코딩 감지 함수 \n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as file:  # 파일을 바이너리 모드로 열기\n",
    "        raw_data = file.read(10000)  # 파일의 첫 부분을 읽어 인코딩 감지 (전체 파일을 읽어도 되지만 메모리를 많이 사용할 수 있음)\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding = result['encoding']\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7917bc24-acd3-468a-a452-8b8fe507b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: Window size 함수 \n",
    "\n",
    "def cut_window_sizes(sentences,key):\n",
    "    # window_size 설정\n",
    "    window_size_get = int(window_cnt_entry.get())\n",
    "\n",
    "    # 콘코던스 설정\n",
    "    concordance_entry_get = track_down_concordance_words()\n",
    "    \n",
    "    if concordance_entry_get=='None':\n",
    "        messagebox.showinfo(\"Information\", \"윈도우 사이즈를 사용하려면, 콘코던스 값을 입력해주세요\")\n",
    "        return \n",
    "        \n",
    "    \n",
    "    # 언어 단위 설정\n",
    "    linguistic_unit_menu_get = linguistic_unit_menu.get()\n",
    "\n",
    "    # 형태소 분석기 설정\n",
    "    morpheme_menu_get = morpheme_menu.get()\n",
    "\n",
    "    # 윈도우 사이즈 리스트 생성\n",
    "    window_size_list = [] \n",
    "    \n",
    "    # 문장 window_size 자르기\n",
    "    for language_sentence in sentences:\n",
    "\n",
    "        if linguistic_unit_menu_get == '어절':\n",
    "            # 문장을 공백 기준으로 단어로 분리\n",
    "            words = language_sentence.split()\n",
    "        \n",
    "        else:\n",
    "            words = morpheme_analyzers[morpheme_menu_get].morphs(language_sentence)\n",
    "        \n",
    "        for concordance in concordance_entry_get:\n",
    "            if concordance in language_sentence:\n",
    "                #print(\"조건 통과\")\n",
    "                try:\n",
    "                    target_index = words.index(concordance)\n",
    "                except ValueError as e:\n",
    "                    messagebox.showinfo(\"Information\",\"콘코던스가 존재하지 않습니다!!!\")        \n",
    "\n",
    "                start_index = max(0, target_index - window_size_get)\n",
    "                end_index = min(len(words), target_index + window_size_get + 1)\n",
    "\n",
    "                window_sentence = ' '.join(words[start_index:end_index])\n",
    "\n",
    "                window_size_list.append(window_sentence)\n",
    "                  \n",
    "    window_size_dataset = pd.DataFrame(window_size_list,columns=['window_size_sentence'])\n",
    "    window_size_dataset.to_csv(f'{key}\\\\window_size.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d099683f-4daa-44b4-bd04-ab9d281afe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_language_unit():\n",
    "    # 언어 단위 설정\n",
    "    linguistic_unit_menu_get = linguistic_unit_menu.get()\n",
    "    #linguistic_unit_menu_get = linguistic_unit_menu\n",
    "\n",
    "    if linguistic_unit_menu_get =='어절':\n",
    "        return separate_sentence_to_phrase()\n",
    "    else:\n",
    "        return tag_part_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "10fc1f24-14d3-4794-bda9-e6a920a7c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_final_language_analyzer():\n",
    "\n",
    "    language_unit,fold_path = choose_language_unit()\n",
    "\n",
    "    senetence_dataset = language_unit\n",
    "\n",
    "    sentences = senetence_dataset['original']\n",
    "    \n",
    "    # 윈도우 사이즈 설정 \n",
    "    window_size_get = window_cnt_entry.get()\n",
    "    if window_size_get != '' :\n",
    "        cut_window_sizes(sentences,fold_path)\n",
    "        \n",
    "        messagebox.showinfo(\"Information\", \"형태소 분석이 완료되었습니다! 결과가 저장되었습니다.\")\n",
    "        \n",
    "        return language_unit\n",
    "    else:\n",
    "        \n",
    "        messagebox.showinfo(\"Information\", \"형태소 분석이 완료되었습니다! 결과가 저장되었습니다.\")\n",
    "        \n",
    "        return language_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d7ebc7b3-e9a2-409f-8476-4e02cf8626f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_numbers(char):\n",
    "    return char.isdigit()\n",
    "\n",
    "def update_user_input(*args):\n",
    "    user_input.delete(0, 'end')\n",
    "    user_input.insert(0, tag_options[tag_variable.get()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "697e0ac2-a8c9-4760-a1b6-585f28c2dd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action: 스타일 설정\n",
    "\n",
    "def configure_styles():\n",
    "    style = ttk.Style()\n",
    "    style.theme_use('clam')  # 클램 테마는 더 현대적인 느낌을 줍니다.\n",
    "    style.configure('TLabel', font=('Arial', 10), background='white')\n",
    "    style.configure('TEntry', font=('Arial', 10), padding=5)\n",
    "    style.configure('TButton', font=('Arial', 10), padding=5)\n",
    "    style.configure('TCombobox', font=('Arial', 10), padding=5)\n",
    "    style.map('TCombobox', fieldbackground=[('readonly', 'white')],\n",
    "              selectbackground=[('readonly', 'white')],\n",
    "              selectforeground=[('readonly', 'black')])\n",
    "    style.configure('TFrame', background='white')  # 프레임 배경색 설정\n",
    "    style.configure('Horizontal.TProgressbar', background='#FA8072')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "990d5921-7dbe-4fdd-bf35-2be7f98356d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_table():\n",
    "    global canvas, word_counter\n",
    "\n",
    "    # Remove previous canvas if exists\n",
    "    if canvas is not None:\n",
    "        canvas.get_tk_widget().pack_forget()\n",
    "        canvas = None\n",
    "\n",
    "    # 표를 비웁니다.\n",
    "    # result_tree.delete(*result_tree.get_children())\n",
    "\n",
    "    # 단어 카운터를 초기화합니다.\n",
    "    word_counter = Counter()\n",
    "\n",
    "    # 프로그레스바를 0으로 초기화합니다.\n",
    "    #progress_bar['value'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e9fb05b5-103d-4612-9270-b09458949392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(ngrams_count):\n",
    "    # 그래프를 그린 canvas 객체\n",
    "    global canvas\n",
    "    \n",
    "    try:\n",
    "        # Get the number of items to display from the entry widget\n",
    "        number_of_items = int(graph_cnt_entry.get())\n",
    "    except ValueError:  # In case of invalid input\n",
    "        messagebox.showerror(\"Error\", \"그래프 표출수를 입력하세요\")\n",
    "        return\n",
    "\n",
    "    # Create new figure\n",
    "    fig = Figure(figsize=(8, 6), dpi=100)\n",
    "\n",
    "    # Add a subplot to the new figure\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    # Get the most common 'number_of_items' words\n",
    "    common_words = ngrams_count[:number_of_items]\n",
    "\n",
    "    # Separate the words and their counts\n",
    "    words = list(common_words['Word'].values) \n",
    "    counts = list(common_words['Frequency'].values) \n",
    "\n",
    "    # Plot the data\n",
    "    ax.bar(words, counts)\n",
    "\n",
    "    # Adjust the x-axis labels\n",
    "    ax.set_xticks(words)\n",
    "    shortened_labels = [label if len(label) <= 10 else label[:10] + \"...\" for label in words]\n",
    "    ax.set_xticklabels(shortened_labels, rotation=45, ha=\"right\", fontsize=8)\n",
    "\n",
    "    def hide_non_integers(x, pos):\n",
    "        if x.is_integer():\n",
    "            return \"{:.0f}\".format(x)\n",
    "        return \"\"\n",
    "\n",
    "    def integer_ticks(ax):\n",
    "        # 현재 Y축의 눈금 위치를 가져옵니다.\n",
    "        ticks = ax.get_yticks()\n",
    "\n",
    "        # 소수점이 포함된 눈금 위치를 제거합니다.\n",
    "        int_ticks = [tick for tick in ticks if tick.is_integer()]\n",
    "\n",
    "        # Y축의 눈금 위치를 정수만 포함하도록 설정합니다.\n",
    "        ax.set_yticks(int_ticks)\n",
    "\n",
    "    # Set y-axis tick labels to show only integers\n",
    "    ax.get_yaxis().set_major_formatter(FuncFormatter(hide_non_integers))\n",
    "    integer_ticks(ax)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Remove previous canvas if exists\n",
    "    if canvas is not None:\n",
    "        canvas.get_tk_widget().pack_forget()\n",
    "        canvas = None\n",
    "\n",
    "    # Create a new tkinter Canvas containing the figure\n",
    "    canvas = FigureCanvasTkAgg(fig, master=graph_frame)\n",
    "    canvas.draw()\n",
    "\n",
    "    # Add the canvas to the Label widget\n",
    "    canvas.get_tk_widget().pack() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bf8be45f-60f6-424c-9618-7cdd39c4a702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path not found in the JSON structure: 'Key not found in the JSON structure.'\n",
      "Path not found in the JSON structure: 'Key not found in the JSON structure.'\n",
      "Path not found in the JSON structure: 'Key not found in the JSON structure.'\n"
     ]
    }
   ],
   "source": [
    "# Action: GUI 생성\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"n-gram 및 형태소 분석기 v1.1\")\n",
    "# 프로그램의 고정된 크기\n",
    "program_width = 670\n",
    "program_height = 850\n",
    "\n",
    "# 화면의 중앙에 프로그램이 위치하도록 좌표를 계산합니다.\n",
    "screen_width = root.winfo_screenwidth()\n",
    "screen_height = root.winfo_screenheight()\n",
    "center_x = int((screen_width - program_width) / 2)\n",
    "center_y = int((screen_height - program_height) / 2)\n",
    "\n",
    "# 프로그램의 위치와 크기를 설정합니다.\n",
    "root.geometry(f'{program_width}x{program_height}+{center_x}+{center_y}')\n",
    "\n",
    "configure_styles()\n",
    "\n",
    "# 숫자 입력 확인을 위한 유효성 검사 커맨드 생성\n",
    "vcmd = root.register(only_numbers)\n",
    "\n",
    "# (1,1)\n",
    "tag_label = tk.Label(root, text=\"* 태그\", anchor='w')\n",
    "tag_label.grid(row=0, column=0, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (1,2)\n",
    "user_input = ttk.Entry(root)\n",
    "user_input.grid(row=0, column=1, sticky='ew', padx=10, pady=5)\n",
    "\n",
    "# (1,3)\n",
    "tag_menu_label = tk.Label(root, text=\"* 태그 선택\", anchor='w')\n",
    "tag_menu_label.grid(row=0, column=2, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (1,4)\n",
    "tag_variable = StringVar(root)\n",
    "tag_variable.trace(\"w\", update_user_input)\n",
    "tag_options = {\n",
    "    \"신문 말뭉치\": \"document.paragraph.form\",\n",
    "    \"일상 대화 말뭉치\": \"document.utterance.form\",\n",
    "    \"직접 입력\": \"\",\n",
    "}\n",
    "tag_menu = ttk.Combobox(root, textvariable=tag_variable, values=list(tag_options.keys()), state='readonly')\n",
    "tag_menu.grid(row=0, column=3, sticky='ew', padx=10, pady=5)\n",
    "tag_menu.set(\"직접 입력\")\n",
    "\n",
    "# (2,1)\n",
    "ngram_cnt_label = tk.Label(root, text=\"* n-gram 사이즈\", anchor='w')\n",
    "ngram_cnt_label.grid(row=1, column=0, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (2,2)\n",
    "ngram_cnt_entry = ttk.Entry(root, validate=\"key\", validatecommand=(vcmd, '%S'))\n",
    "ngram_cnt_entry.grid(row=1, column=1, sticky='ew', padx=10, pady=5)\n",
    "ngram_cnt_entry.insert(0, 2)\n",
    "\n",
    "# (2,3)\n",
    "tag_menu_label = tk.Label(root, text=\"* 형태소 분석기\", anchor='w')\n",
    "tag_menu_label.grid(row=1, column=2, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (2,4)\n",
    "morpheme_analyzer = StringVar(root)\n",
    "morpheme_menu = ttk.Combobox(root, textvariable=morpheme_analyzer, values=list(morpheme_analyzers.keys()), state='readonly')\n",
    "morpheme_menu.grid(row=1, column=3, sticky='ew', padx=10, pady=5)\n",
    "morpheme_menu.set(\"Mecab\")\n",
    "\n",
    "# (3,1)\n",
    "filter_key_label = tk.Label(root, text=\"필터키\", anchor='w')\n",
    "filter_key_label.grid(row=2, column=0, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (3,2)\n",
    "filter_key_entry = ttk.Entry(root)\n",
    "filter_key_entry.grid(row=2, column=1, sticky='ew', padx=10, pady=5)\n",
    "\n",
    "# (3,3)\n",
    "tag_menu_label = tk.Label(root, text=\"필터값\", anchor='w')\n",
    "tag_menu_label.grid(row=2, column=2, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (3,4)\n",
    "filter_value_entry = ttk.Entry(root)\n",
    "filter_value_entry.grid(row=2, column=3, sticky='ew', padx=10, pady=5)\n",
    "\n",
    "# (4,1)\n",
    "concordance_label = tk.Label(root, text=\"콘코던스 단어(|로 구분)\", anchor='w')\n",
    "concordance_label.grid(row=3, column=0, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (4,2)\n",
    "concordance_entry = ttk.Entry(root)\n",
    "concordance_entry.grid(row=3, column=1, sticky='ew', padx=10, pady=5)\n",
    "\n",
    "# (4,3)\n",
    "exclude_words_label = tk.Label(root, text=\"제외 단어(|로 구분)\", anchor='w')\n",
    "exclude_words_label.grid(row=3, column=2, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (4,4)\n",
    "exclude_words_entry = ttk.Entry(root)\n",
    "exclude_words_entry.grid(row=3, column=3, sticky='ew', padx=10, pady=5)\n",
    "\n",
    "# (5,1)\n",
    "window_cnt_label = tk.Label(root, text=\"window 사이즈\", anchor='w')\n",
    "window_cnt_label.grid(row=4, column=0, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (5,2)\n",
    "window_cnt_entry = ttk.Entry(root, validate=\"key\", validatecommand=(vcmd, '%S'))\n",
    "window_cnt_entry.grid(row=4, column=1, sticky='ew', padx=10, pady=5)\n",
    "window_cnt_entry.insert(0, \"\")\n",
    "\n",
    "# (5,3)\n",
    "linguistic_unit_label = tk.Label(root, text=\"* 언어 단위 선택\", anchor='w')\n",
    "linguistic_unit_label.grid(row=4, column=2, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (5,4)\n",
    "linguistic_unit_variable = StringVar(root)\n",
    "linguistic_unit_variable.trace(\"w\", update_user_input)\n",
    "linguistic_unit_options = {\n",
    "    \"형태소\": \"\",\n",
    "    \"어절\": \"\"}\n",
    "linguistic_unit_menu = ttk.Combobox(root, textvariable=linguistic_unit_variable, values=list(linguistic_unit_options.keys()), state='readonly')\n",
    "linguistic_unit_menu.grid(row=4, column=3, sticky='ew', padx=10, pady=5)\n",
    "linguistic_unit_menu.set(\"형태소\")\n",
    "\n",
    "# (6,1)\n",
    "graph_cnt_label = tk.Label(root, text=\"* 그래프 표출수(최대20)\", anchor='w')\n",
    "graph_cnt_label.grid(row=5, column=0, sticky='we', padx=10, pady=5)\n",
    "\n",
    "#(6,2)\n",
    "graph_cnt_entry = ttk.Entry(root, validate=\"key\", validatecommand=(vcmd, '%S'))\n",
    "graph_cnt_entry.grid(row=5, column=1, sticky='ew', padx=10, pady=5)\n",
    "graph_cnt_entry.insert(0, \"10\")\n",
    "\n",
    "# (6,3)\n",
    "extract_cnt_label = tk.Label(root, text=\"표 표출수\", anchor='w')\n",
    "extract_cnt_label.grid(row=5, column=2, sticky='we', padx=10, pady=5)\n",
    "\n",
    "# (6,4)\n",
    "extract_cnt_entry = ttk.Entry(root, validate=\"key\", validatecommand=(vcmd, '%S'))\n",
    "extract_cnt_entry.grid(row=5, column=3, sticky='ew', padx=10, pady=5)\n",
    "extract_cnt_entry.insert(0,0)\n",
    "\n",
    "# (7,1)\n",
    "folder_button = ttk.Button(root, text=\"폴더 선택 및 분석\", command=select_final_language_analyzer)\n",
    "folder_button.grid(row=6, column=0, sticky='ew', padx=10, pady=5, columnspan=2)\n",
    "\n",
    "# (7,2)\n",
    "reset_button = ttk.Button(root, text=\"결과창 리셋\", command=reset_table)\n",
    "reset_button.grid(row=6, column=2, sticky='ew', padx=10, pady=5, columnspan=2)\n",
    "\n",
    "# (8,1)\n",
    "graph_frame = ttk.Frame(root, height=450)  # height를 설정해 줍니다.\n",
    "graph_frame.grid(row=7, column=0, columnspan=4, sticky='ew')  # sticky를 'ew'로 변경합니다.\n",
    "\n",
    "root.grid_columnconfigure(1, weight=1)\n",
    "root.grid_rowconfigure(6, weight=1)\n",
    "root.grid_rowconfigure(7, weight=1)\n",
    "\n",
    "# 초기 캔버스 설정을 None으로 합니다. 'embed_figure' 함수 정의가 필요합니다.\n",
    "canvas = None\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1c163c32-e189-412b-b682-7f28261ab815",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\pc021\\Desktop\\JASON\\count.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6d8fd90e-4e0c-49ba-b10d-4d0bc5aab94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_cloud(data):\n",
    "\n",
    "    base_path = os.getcwd()\n",
    "    font_path = os.path.join(base_path, \"fonts\", \"malgun.ttf\")\n",
    "    \n",
    "    N = 20\n",
    "    top_n_df = data.nlargest(N, 'Frequency')\n",
    "\n",
    "    word_freq = dict(zip(top_n_df['Word'], top_n_df['Frequency']))\n",
    "    \n",
    "    # 워드 클라우드 생성\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white',font_path = font_path).generate_from_frequencies(word_freq)\n",
    "\n",
    "    # 워드 클라우드 출력\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')  # 축 제거\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e6022682-1bf6-42ae-95f0-0e71e541a2d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Word1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_38\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_38\\lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_38\\lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Word1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[170], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 엣지 추가\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m---> 10\u001b[0m     G\u001b[38;5;241m.\u001b[39madd_edge(\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWord1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord2\u001b[39m\u001b[38;5;124m'\u001b[39m], weight\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 네트워크 차트 시각화\u001b[39;00m\n\u001b[0;32m     13\u001b[0m pos \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mspring_layout(G, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)  \u001b[38;5;66;03m# 레이아웃 설정\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_38\\lib\\site-packages\\pandas\\core\\series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1012\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_38\\lib\\site-packages\\pandas\\core\\series.py:1116\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1116\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_38\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Word1'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 네트워크 그래프 생성\n",
    "G = nx.Graph()\n",
    "\n",
    "# 엣지 추가\n",
    "for _, row in df.iterrows():\n",
    "    G.add_edge(row['Word1'], row['Word2'], weight=row['Frequency'])\n",
    "\n",
    "# 네트워크 차트 시각화\n",
    "pos = nx.spring_layout(G, seed=42)  # 레이아웃 설정\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# 노드와 엣지 그리기\n",
    "nx.draw_networkx_nodes(G, pos, node_size=500, node_color='skyblue')\n",
    "nx.draw_networkx_edges(G, pos, width=[G[u][v]['weight']*0.01 for u, v in G.edges()], alpha=0.6)\n",
    "nx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')\n",
    "\n",
    "# 엣지 라벨 그리기 (빈도수)\n",
    "edge_labels = {(u, v): d['weight'] for u, v, d in G.edges(data=True)}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "\n",
    "plt.title('Word Pair Network')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_38",
   "language": "python",
   "name": "python_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
